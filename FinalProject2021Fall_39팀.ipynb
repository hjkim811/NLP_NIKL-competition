{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: 2021년 국립국어원 인공지능 언어능력 평가\n",
    "\n",
    "- [2021년 국립국어원 인공지능 언어능력 평가](https://corpus.korean.go.kr/task/taskList.do?taskId=1&clCd=END_TASK&subMenuId=sub01) 는 9월 1일부터 시작하여 11월 1일까지 마감된 [네 가지 과제에](https://corpus.korean.go.kr/task/taskDownload.do?taskId=1&clCd=END_TASK&subMenuId=sub02) 대한 언어능력 평가 대회\n",
    "- 여기서 제시된 과제를 그대로 수행하여 그 결과를 [최종 선정된 결과들](https://corpus.korean.go.kr/task/taskLeaderBoard.do?taskId=4&clCd=END_TASK&subMenuId=sub04)과 비교할 수 있도록 수행\n",
    "- 아직 테스트 셋의 정답이 공식적으로 공개되고 있지 않아, 네 가지 과제의 자료에서 evaluation dataset으로 가지고 성능을 비교할 계획\n",
    "- 기말 발표전까지 정답셋이 공개될 경우 이 정답셋을 가지고 성능 검증\n",
    "- Transformers 기반 방법론, 신경망 등 각자 생각한 방법대로 구현 가능\n",
    "- 현재 대회기간이 종료되어 자료가 다운로드 가능하지 않으니 첨부된 자료 참조\n",
    "- 개인적으로 하거나 최대 두명까지 그룹 허용. \n",
    "- 이 노트북 화일에 이름을 변경하여 작업하고 제출. 제출시 화일명을 FinalProject_[DS또는 CL]_학과_이름.ipynb\n",
    "- 마감 12월 6일(월) 23:59분까지.\n",
    "- 12월 7일, 9일 기말 발표 presentation 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리더보드\n",
    "\n",
    "- 최종발표전까지 각조는 각 태스크별 실행성능을 **시도된 여러 방법의 결과들을 지속적으로**  [리더보드](https://docs.google.com/spreadsheets/d/1-uenfp5GolpY2Gf0TsFbODvj585IIiFKp9fvYxcfgkY/edit#gid=0)에 해당 팀명(구성원 이름 포함)을 입력하여 공개하여야 함. \n",
    "- 최종 마감일에 이 순위와 실제 제출한 프로그램의 수행 결과를 비교하여 성능을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 문법성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('./data//NIKL_CoLA_train.tsv', sep='\\t')\n",
    "dev = pd.read_csv('./data/NIKL_CoLA_dev.tsv', sep='\\t')\n",
    "test = pd.read_csv('./data/NIKL_CoLA_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence = dataset['sentence'].tolist()\n",
    "\n",
    "    tokenized_sentences = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=37,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids = True\n",
    "        )\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.1, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        if self.use_activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, pretrained, dropout_rate):\n",
    "        super(Attention, self).__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.classifier = nn.Sequential(\n",
    "            FCLayer(256, 256, dropout_rate=dropout_rate),\n",
    "            FCLayer(256, 256, dropout_rate=dropout_rate),\n",
    "            FCLayer(256, 128, dropout_rate=dropout_rate),\n",
    "            FCLayer(128, 2)\n",
    "        )\n",
    "        self.Q = nn.Linear(768,256)\n",
    "        self.K = nn.Linear(768,256)\n",
    "        self.V = nn.Linear(768,256)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None):\n",
    "        output = self.pretrained(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                                 token_type_ids=token_type_ids)[0]\n",
    "        \n",
    "        d_k = self.K(output).size(-1)\n",
    "        attention_score  = torch.matmul(self.Q(output), torch.transpose(self.K(output), 1, 2))\n",
    "        attention_score = attention_score / math.sqrt(d_k)\n",
    "        \n",
    "        mask = torch.nan_to_num((attention_mask - 1) * np.inf, 0)\n",
    "        attn_dim = attention_mask.shape[-1]\n",
    "        mask = mask.repeat(1, attn_dim).reshape(-1, attn_dim, attn_dim)\n",
    "        \n",
    "        attention_score += mask\n",
    "        score = F.softmax(attention_score, -1)\n",
    "        output = torch.matmul(score, self.V(output))\n",
    "        output = torch.mean(output, 1)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train, dev, model_name, batch_size, num_epochs, learning_rate,\n",
    "             saved_folder, drop_rate=0.2, load_model=None, fix=False):\n",
    "    path = 'monologg/koelectra-base-v3-discriminator'\n",
    "    warmup_ratio = 0.1\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    tok = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "    model = eval('%s(model.electra, drop_rate)' % model_name)\n",
    "    \n",
    "    if load_model is not None:\n",
    "        model.load_state_dict(torch.load(load_model))\n",
    "    \n",
    "    # Freeze Parameter\n",
    "    if fix:\n",
    "        for name, param in model.named_parameters():\n",
    "            if ('electra' in name):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    tokenized_train = tokenized_dataset(train, tok)\n",
    "    train_label = train['acceptability_label'].values\n",
    "\n",
    "    tokenized_dev = tokenized_dataset(dev, tok)\n",
    "    dev_label = dev['acceptability_label'].values\n",
    "\n",
    "    train_dataset = CustomDataset(tokenized_train, train_label)\n",
    "    dev_dataset = CustomDataset(tokenized_dev, dev_label)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    t_total = len(train_loader) * num_epochs\n",
    "    warmup_step = int(t_total * warmup_ratio)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=warmup_step, \n",
    "                                                num_training_steps=t_total)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_loader, dynamic_ncols=True)\n",
    "\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_y = []\n",
    "\n",
    "        for idx, items in enumerate(pbar):\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outs = model(**item)\n",
    "            loss = criterion(outs, item['labels'])\n",
    "\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            labels = items['labels'].cpu().detach().numpy().tolist()\n",
    "            preds = preds.cpu().detach().numpy().tolist()\n",
    "            train_pred += labels\n",
    "            train_y += preds\n",
    "\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            pbar = tqdm(dev_loader, dynamic_ncols=True)\n",
    "            dev_pred = []\n",
    "            dev_y = []\n",
    "\n",
    "\n",
    "            for idx, items in enumerate(pbar):\n",
    "                item = {key: val.to(device) for key, val in items.items()}\n",
    "                outs = model(**item)\n",
    "                preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "                labels = items['labels'].cpu().detach().numpy().tolist()\n",
    "                preds = preds.cpu().detach().numpy().tolist()\n",
    "                dev_pred += labels\n",
    "                dev_y += preds\n",
    "\n",
    "\n",
    "        train_acc = accuracy_score(train_pred, train_y)\n",
    "        dev_acc = accuracy_score(dev_pred, dev_y)\n",
    "        train_mcc = matthews_corrcoef(train_pred, train_y)\n",
    "        dev_mcc = matthews_corrcoef(dev_pred, dev_y)\n",
    "\n",
    "        print('train')\n",
    "        print('acc: %.4f, mcc: %.4f' % (train_acc, train_mcc))\n",
    "        print('dev')\n",
    "        print('acc: %.4f, mcc: %.4f' % (dev_acc, dev_mcc))\n",
    "\n",
    "\n",
    "        os.makedirs(saved_folder, exist_ok=True)\n",
    "        files = glob(os.path.join(saved_folder, '%s_%s*' % (path.split('/')[1], model_name)))\n",
    "\n",
    "        if len(files) != 0:\n",
    "            if np.max([float(f.split('_')[-1][:-3]) for f in files]) < dev_mcc:\n",
    "\n",
    "                name = '%s_%s_%.6f.pt' % (path.split('/')[1], model_name, dev_mcc)\n",
    "                torch.save(model.state_dict(), os.path.join(saved_folder, name))\n",
    "        else:\n",
    "            name = '%s_%s_%.6f.pt' % (path.split('/')[1], model_name, dev_mcc)\n",
    "            torch.save(model.state_dict(), os.path.join(saved_folder, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dev, model_name, load_model=None):\n",
    "    path = 'monologg/koelectra-base-v3-discriminator'\n",
    "    warmup_ratio = 0.1\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    tok = AutoTokenizer.from_pretrained(path)\n",
    "    \n",
    "    model = eval('%s(model.electra, 0)' % model_name)\n",
    "    \n",
    "    if load_model is not None:\n",
    "        model.load_state_dict(torch.load(load_model))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    tokenized_dev = tokenized_dataset(dev, tok)\n",
    "    dev_label = dev['acceptability_label'].values\n",
    "\n",
    "    dev_dataset = CustomDataset(tokenized_dev, dev_label)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pbar = tqdm(dev_loader, dynamic_ncols=True)\n",
    "        dev_pred = []\n",
    "        dev_y = []\n",
    "\n",
    "\n",
    "        for idx, items in enumerate(pbar):\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "            outs = model(**item)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            labels = items['labels'].cpu().detach().numpy().tolist()\n",
    "            preds = preds.cpu().detach().numpy().tolist()\n",
    "            dev_pred += labels\n",
    "            dev_y += preds\n",
    "\n",
    "    dev_acc = accuracy_score(dev_pred, dev_y)\n",
    "    dev_mcc = matthews_corrcoef(dev_pred, dev_y)\n",
    "\n",
    "    print('dev')\n",
    "    print('acc: %.4f, mcc: %.4f' % (dev_acc, dev_mcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Attention'\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "learning_rate =  0.00002\n",
    "warmup_ratio = 0.1\n",
    "saved_folder = 'saved_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(train, dev, 'Attention', batch_size, num_epochs, learning_rate, \n",
    "         saved_folder, fix=False, load_model=None, drop_rate=0.5)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 8/8 [00:01<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "acc: 0.7894, mcc: 0.5773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "load_model = 'saved_model/문법성_0.577342.pt'\n",
    "inference(dev, 'Attention', load_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 동형이의어구별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Training Data (csv to jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import random\n",
    "import array\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, AutoModelWithLMHead, AutoModelForPreTraining\n",
    "from transformers import ElectraModel, ElectraTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '../dataset/2. 동형이의어구별'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(f'{my_path}/NIKL_SKT_WiC_Train.tsv', sep=\"\\t\")\n",
    "df_val = pd.read_csv(f'{my_path}/NIKL_SKT_WiC_Dev.tsv', sep=\"\\t\")\n",
    "df_test = pd.read_csv(f'{my_path}/NIKL_SKT_WiC_Test.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>SENTENCE1</th>\n",
       "      <th>SENTENCE2</th>\n",
       "      <th>ANSWER</th>\n",
       "      <th>start_s1</th>\n",
       "      <th>end_s1</th>\n",
       "      <th>start_s2</th>\n",
       "      <th>end_s2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>단정</td>\n",
       "      <td>그의 죽음은 타살로 단정이 되었다.</td>\n",
       "      <td>단정이 된 교실은 정돈되어 있다.</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>단수</td>\n",
       "      <td>현대 생활에서 단전과 단수의 고통은 겪어 보지 않으면 짐작도 못한다.</td>\n",
       "      <td>사업자를 단수로 할지 복수로 할지를 놓고 관계자들 사이에 입씨름이 벌어졌다.</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>화성</td>\n",
       "      <td>화성은 밤과 낮, 하루의 길이와 계절의 변화가 지구와 매우 비슷하다.</td>\n",
       "      <td>화성은 서양 음악을 이루는 중요한 요소이다.</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>자전</td>\n",
       "      <td>달의 자전 주기는 달이 지구의 둘레를 공전하는 주기와 같다.</td>\n",
       "      <td>태양계의 모든 행성은 자전을 한다.</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>동지</td>\n",
       "      <td>오늘의 적이 내일은 동지가 될 수 있다.</td>\n",
       "      <td>동지에 무와 감자를 나누어 심었다.</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7743</th>\n",
       "      <td>7744</td>\n",
       "      <td>맞</td>\n",
       "      <td>고 노무현 전 대통령의 딸 정연 씨가 25일 “2009년 미국 아파트 원주인인 경연...</td>\n",
       "      <td>최루탄 머리 맞고 9개월 혼수상태… ‘터키 민주화시위 상징’으로 떠올라</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7744</th>\n",
       "      <td>7745</td>\n",
       "      <td>뜨</td>\n",
       "      <td>아들은 \"50년 넘게 집에서만 지내신 어머니가 이번에야말로 눈을 뜨면 좋겠다\"고 기...</td>\n",
       "      <td>“안철수 원장이 정치권에서 뜨는 것은 사회적으로 잘못된 것이다.”(심대평 자유선진당...</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7745</th>\n",
       "      <td>7746</td>\n",
       "      <td>보였</td>\n",
       "      <td>시내 대부분은 폐허가 됐고, 무너지지 않은 호텔은 이미 방이 꽉 차 있어 마땅히 숙...</td>\n",
       "      <td>양원준(41·사진) 복지TV 기획팀장이 오른쪽 와이셔츠 소매를 걷어올리자 100원짜...</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>79</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7746</th>\n",
       "      <td>7747</td>\n",
       "      <td>불리</td>\n",
       "      <td>미방위원인 최민희 새정치연합 의원은 언론개혁시민연대와 함께 국회에서 기자회견을 열어...</td>\n",
       "      <td>‘축구 종가’ 잉글랜드의 안방으로 ‘축구의 성지’로 불리는 웸블리구장은 9만 관중을...</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7747</th>\n",
       "      <td>7748</td>\n",
       "      <td>맞</td>\n",
       "      <td>북한에 유리한 기권 결정을 내리고 그 같은 사실을 북측에 알렸는데, 북한이 '찬성 ...</td>\n",
       "      <td>경북 경주 보문단지에 있는 ‘경주 캘리포니아 비치’는 올해 개장 6년째를 맞아 90...</td>\n",
       "      <td>False</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7748 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID Target                                          SENTENCE1  \\\n",
       "0        1     단정                                그의 죽음은 타살로 단정이 되었다.   \n",
       "1        2     단수             현대 생활에서 단전과 단수의 고통은 겪어 보지 않으면 짐작도 못한다.   \n",
       "2        3     화성             화성은 밤과 낮, 하루의 길이와 계절의 변화가 지구와 매우 비슷하다.   \n",
       "3        4     자전                  달의 자전 주기는 달이 지구의 둘레를 공전하는 주기와 같다.   \n",
       "4        5     동지                             오늘의 적이 내일은 동지가 될 수 있다.   \n",
       "...    ...    ...                                                ...   \n",
       "7743  7744      맞  고 노무현 전 대통령의 딸 정연 씨가 25일 “2009년 미국 아파트 원주인인 경연...   \n",
       "7744  7745      뜨  아들은 \"50년 넘게 집에서만 지내신 어머니가 이번에야말로 눈을 뜨면 좋겠다\"고 기...   \n",
       "7745  7746     보였  시내 대부분은 폐허가 됐고, 무너지지 않은 호텔은 이미 방이 꽉 차 있어 마땅히 숙...   \n",
       "7746  7747     불리  미방위원인 최민희 새정치연합 의원은 언론개혁시민연대와 함께 국회에서 기자회견을 열어...   \n",
       "7747  7748      맞  북한에 유리한 기권 결정을 내리고 그 같은 사실을 북측에 알렸는데, 북한이 '찬성 ...   \n",
       "\n",
       "                                              SENTENCE2  ANSWER  start_s1  \\\n",
       "0                                    단정이 된 교실은 정돈되어 있다.   False        11   \n",
       "1            사업자를 단수로 할지 복수로 할지를 놓고 관계자들 사이에 입씨름이 벌어졌다.   False        12   \n",
       "2                              화성은 서양 음악을 이루는 중요한 요소이다.   False         0   \n",
       "3                                   태양계의 모든 행성은 자전을 한다.    True         3   \n",
       "4                                   동지에 무와 감자를 나누어 심었다.   False        11   \n",
       "...                                                 ...     ...       ...   \n",
       "7743            최루탄 머리 맞고 9개월 혼수상태… ‘터키 민주화시위 상징’으로 떠올라   False       100   \n",
       "7744  “안철수 원장이 정치권에서 뜨는 것은 사회적으로 잘못된 것이다.”(심대평 자유선진당...   False        36   \n",
       "7745  양원준(41·사진) 복지TV 기획팀장이 오른쪽 와이셔츠 소매를 걷어올리자 100원짜...    True        77   \n",
       "7746  ‘축구 종가’ 잉글랜드의 안방으로 ‘축구의 성지’로 불리는 웸블리구장은 9만 관중을...   False        94   \n",
       "7747  경북 경주 보문단지에 있는 ‘경주 캘리포니아 비치’는 올해 개장 6년째를 맞아 90...   False        67   \n",
       "\n",
       "      end_s1  start_s2  end_s2  \n",
       "0         13         0       2  \n",
       "1         14         5       7  \n",
       "2          2         0       2  \n",
       "3          5        12      14  \n",
       "4         13         0       2  \n",
       "...      ...       ...     ...  \n",
       "7743     101         7       8  \n",
       "7744      37        15      16  \n",
       "7745      79        58      60  \n",
       "7746      96        29      31  \n",
       "7747      68        41      42  \n",
       "\n",
       "[7748 rows x 9 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편의를 위해 영어 WiC 데이터와 같은 이름으로\n",
    "# ID -> idx로는 안 바꿈\n",
    "df_list = [df_train, df_val, df_test]\n",
    "\n",
    "for df in df_list:\n",
    "    df.rename(columns = {'Target':'word', \n",
    "                         'SENTENCE1':'sentence1',\n",
    "                         'SENTENCE2':'sentence2',\n",
    "                         'ANSWER':'label',\n",
    "                         'start_s1':'start1',\n",
    "                         'start_s2':'start2',\n",
    "                         'end_s1':'end1',\n",
    "                         'end_s2':'end2'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>start1</th>\n",
       "      <th>end1</th>\n",
       "      <th>start2</th>\n",
       "      <th>end2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>단정</td>\n",
       "      <td>그의 죽음은 타살로 단정이 되었다.</td>\n",
       "      <td>단정이 된 교실은 정돈되어 있다.</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>단수</td>\n",
       "      <td>현대 생활에서 단전과 단수의 고통은 겪어 보지 않으면 짐작도 못한다.</td>\n",
       "      <td>사업자를 단수로 할지 복수로 할지를 놓고 관계자들 사이에 입씨름이 벌어졌다.</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>화성</td>\n",
       "      <td>화성은 밤과 낮, 하루의 길이와 계절의 변화가 지구와 매우 비슷하다.</td>\n",
       "      <td>화성은 서양 음악을 이루는 중요한 요소이다.</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>자전</td>\n",
       "      <td>달의 자전 주기는 달이 지구의 둘레를 공전하는 주기와 같다.</td>\n",
       "      <td>태양계의 모든 행성은 자전을 한다.</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>동지</td>\n",
       "      <td>오늘의 적이 내일은 동지가 될 수 있다.</td>\n",
       "      <td>동지에 무와 감자를 나누어 심었다.</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7743</th>\n",
       "      <td>7744</td>\n",
       "      <td>맞</td>\n",
       "      <td>고 노무현 전 대통령의 딸 정연 씨가 25일 “2009년 미국 아파트 원주인인 경연...</td>\n",
       "      <td>최루탄 머리 맞고 9개월 혼수상태… ‘터키 민주화시위 상징’으로 떠올라</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7744</th>\n",
       "      <td>7745</td>\n",
       "      <td>뜨</td>\n",
       "      <td>아들은 \"50년 넘게 집에서만 지내신 어머니가 이번에야말로 눈을 뜨면 좋겠다\"고 기...</td>\n",
       "      <td>“안철수 원장이 정치권에서 뜨는 것은 사회적으로 잘못된 것이다.”(심대평 자유선진당...</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7745</th>\n",
       "      <td>7746</td>\n",
       "      <td>보였</td>\n",
       "      <td>시내 대부분은 폐허가 됐고, 무너지지 않은 호텔은 이미 방이 꽉 차 있어 마땅히 숙...</td>\n",
       "      <td>양원준(41·사진) 복지TV 기획팀장이 오른쪽 와이셔츠 소매를 걷어올리자 100원짜...</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>79</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7746</th>\n",
       "      <td>7747</td>\n",
       "      <td>불리</td>\n",
       "      <td>미방위원인 최민희 새정치연합 의원은 언론개혁시민연대와 함께 국회에서 기자회견을 열어...</td>\n",
       "      <td>‘축구 종가’ 잉글랜드의 안방으로 ‘축구의 성지’로 불리는 웸블리구장은 9만 관중을...</td>\n",
       "      <td>False</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7747</th>\n",
       "      <td>7748</td>\n",
       "      <td>맞</td>\n",
       "      <td>북한에 유리한 기권 결정을 내리고 그 같은 사실을 북측에 알렸는데, 북한이 '찬성 ...</td>\n",
       "      <td>경북 경주 보문단지에 있는 ‘경주 캘리포니아 비치’는 올해 개장 6년째를 맞아 90...</td>\n",
       "      <td>False</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7748 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID word                                          sentence1  \\\n",
       "0        1   단정                                그의 죽음은 타살로 단정이 되었다.   \n",
       "1        2   단수             현대 생활에서 단전과 단수의 고통은 겪어 보지 않으면 짐작도 못한다.   \n",
       "2        3   화성             화성은 밤과 낮, 하루의 길이와 계절의 변화가 지구와 매우 비슷하다.   \n",
       "3        4   자전                  달의 자전 주기는 달이 지구의 둘레를 공전하는 주기와 같다.   \n",
       "4        5   동지                             오늘의 적이 내일은 동지가 될 수 있다.   \n",
       "...    ...  ...                                                ...   \n",
       "7743  7744    맞  고 노무현 전 대통령의 딸 정연 씨가 25일 “2009년 미국 아파트 원주인인 경연...   \n",
       "7744  7745    뜨  아들은 \"50년 넘게 집에서만 지내신 어머니가 이번에야말로 눈을 뜨면 좋겠다\"고 기...   \n",
       "7745  7746   보였  시내 대부분은 폐허가 됐고, 무너지지 않은 호텔은 이미 방이 꽉 차 있어 마땅히 숙...   \n",
       "7746  7747   불리  미방위원인 최민희 새정치연합 의원은 언론개혁시민연대와 함께 국회에서 기자회견을 열어...   \n",
       "7747  7748    맞  북한에 유리한 기권 결정을 내리고 그 같은 사실을 북측에 알렸는데, 북한이 '찬성 ...   \n",
       "\n",
       "                                              sentence2  label  start1  end1  \\\n",
       "0                                    단정이 된 교실은 정돈되어 있다.  False      11    13   \n",
       "1            사업자를 단수로 할지 복수로 할지를 놓고 관계자들 사이에 입씨름이 벌어졌다.  False      12    14   \n",
       "2                              화성은 서양 음악을 이루는 중요한 요소이다.  False       0     2   \n",
       "3                                   태양계의 모든 행성은 자전을 한다.   True       3     5   \n",
       "4                                   동지에 무와 감자를 나누어 심었다.  False      11    13   \n",
       "...                                                 ...    ...     ...   ...   \n",
       "7743            최루탄 머리 맞고 9개월 혼수상태… ‘터키 민주화시위 상징’으로 떠올라  False     100   101   \n",
       "7744  “안철수 원장이 정치권에서 뜨는 것은 사회적으로 잘못된 것이다.”(심대평 자유선진당...  False      36    37   \n",
       "7745  양원준(41·사진) 복지TV 기획팀장이 오른쪽 와이셔츠 소매를 걷어올리자 100원짜...   True      77    79   \n",
       "7746  ‘축구 종가’ 잉글랜드의 안방으로 ‘축구의 성지’로 불리는 웸블리구장은 9만 관중을...  False      94    96   \n",
       "7747  경북 경주 보문단지에 있는 ‘경주 캘리포니아 비치’는 올해 개장 6년째를 맞아 90...  False      67    68   \n",
       "\n",
       "      start2  end2  \n",
       "0          0     2  \n",
       "1          5     7  \n",
       "2          0     2  \n",
       "3         12    14  \n",
       "4          0     2  \n",
       "...      ...   ...  \n",
       "7743       7     8  \n",
       "7744      15    16  \n",
       "7745      58    60  \n",
       "7746      29    31  \n",
       "7747      41    42  \n",
       "\n",
       "[7748 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl 파일로 저장\n",
    "df_train.to_json(f'{my_path}/WiC_train.jsonl', orient='records', lines=True, force_ascii = False)\n",
    "df_val.to_json(f'{my_path}/WiC_val.jsonl', orient='records', lines=True, force_ascii = False)\n",
    "df_test.to_json(f'{my_path}/WiC_test.jsonl', orient='records', lines=True, force_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tunib/electra-ko-base were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# klue/bert-base\n",
    "# model = AutoModel.from_pretrained(\"klue/bert-base\")\n",
    "# model_name = 'BERT-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "# klue/roberta-base\n",
    "# model = AutoModel.from_pretrained(\"klue/roberta-base\")\n",
    "# model_name = 'RoBERTa-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "\n",
    "# klue/roberta-large\n",
    "# model = AutoModel.from_pretrained(\"klue/roberta-large\")\n",
    "# model_name = 'RoBERTa-large'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "# monlogg/koelectra-base-v2-discriminator\n",
    "# model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v2-discriminator\")\n",
    "# model_name = 'KoELECTRA-base-v2-dis'\n",
    "# tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v2-discriminator\")\n",
    "\n",
    "# monlogg/koelectra-base-v3-discriminator\n",
    "# model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "# model_name = 'KoELECTRA-base-v3-dis'\n",
    "# tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "\n",
    "# tunib/electra-ko-base\n",
    "model = AutoModel.from_pretrained('tunib/electra-ko-base')\n",
    "model_name = 'KoELECTRA-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained('tunib/electra-ko-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 128, 3]\n",
      "[2, 13843, 13012, 6044, 3]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer 체크 (앞뒤에 CLS, SEP 있는지)\n",
    "print(tokenizer.encode(\"가\"))\n",
    "print(tokenizer.encode(\"여행을 떠나자\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA TITAN V'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determine how many elements we want to train during each iteration\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "PATIENCE = 10\n",
    "\n",
    "# Prepare Torch to use GPU, and use CPU when it's not available\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\")\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "\n",
    "#Get the GPU device name\n",
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Readfile function to take all the objects out of jsonl files\n",
    "def parse_file_to_JSON(filename):\n",
    "    serparated_json_objs = []\n",
    "    \n",
    "    #grab each line and add it as an element in json objs arr\n",
    "    with open(filename, mode = \"r\") as jsonl_file:\n",
    "        for i in jsonl_file:\n",
    "            serparated_json_objs.append(json.loads(i))\n",
    "\n",
    "    return serparated_json_objs\n",
    "\n",
    "#Take a list of words (strings) and a sentence and returns a list\n",
    "#of pairs indicating the tokens' start and end positions in the sentence for each word\n",
    "#Create a function that matches word in tokenized sentence\n",
    "def find_word_in_tokenized_sentence(word,token_ids):\n",
    "    \n",
    "    # decomposedWord = tokenizer.encode(word)\n",
    "    decomposedWord = tokenizer.encode(word)[1:-1] # 수정함\n",
    "    \n",
    "    # print('decomposedWord:', decomposedWord) # 추가함\n",
    "   #Iterate through to find a matching sublist of the token_ids\n",
    "    for i in range(len(token_ids)):\n",
    "        if token_ids[i] == decomposedWord[0] and token_ids[i:i+len(decomposedWord)] == decomposedWord:\n",
    "            return (i,i+len(decomposedWord)-1)\n",
    "    #finalize the output if there is no matching pattern found\n",
    "    # print('returned -1 -1') # 추가함\n",
    "    return (-1,-1)\n",
    "  \n",
    "def find_words_in_tokenized_sentences(wordList,token_ids):\n",
    "    #Create a intList that marks the positions of words\n",
    "    intList = []\n",
    "    #if intList is empty, call the previous function as no matching pattern found\n",
    "    for word in wordList:\n",
    "        if len(intList) == 0:\n",
    "            intList.append(find_word_in_tokenized_sentence(word,token_ids))\n",
    "        else:\n",
    "            afterLastInterval = intList[-1][1]+1\n",
    "            interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:])\n",
    "            actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n",
    "            intList.append(actualPositions)\n",
    "    return intList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(json_objects, training = True):\n",
    "    \n",
    "    wic_sentences, wic_encoded, wic_labels, wic_word_locs, wic_indexes = [], [], [] ,[] ,[]\n",
    "    \n",
    "    for index, example in enumerate(json_objects):\n",
    "        \n",
    "        wic_indexes.append(index)\n",
    "        sentence = f\"<s>{example['sentence1']}</s><s>{example['sentence2']}</s>\"\n",
    "        wic_sentences.append(sentence)\n",
    "\n",
    "        wic_encoded.append(tokenizer.encode(sentence))\n",
    "        \n",
    "        # locate word in context\n",
    "        word = example['word']\n",
    "        location_of_word = (-1, -1)\n",
    "        sent1_split = example['sentence1'].split(' ')\n",
    "        sent2_split = example['sentence2'].split(' ')\n",
    "        \n",
    "        # wic indx\n",
    "        sent1_word_char_loc = (example['start1'], example['end1'])\n",
    "        sent2_word_char_loc = (example['start2'], example['end2'])\n",
    "        \n",
    "        num_characters = 0\n",
    "        \n",
    "        i, j = 0, 0\n",
    "        word1_not_found, word2_not_found = True, True\n",
    "        \n",
    "        #locate word one\n",
    "        while word1_not_found and i < len(sent1_split):\n",
    "            word_len = len(sent1_split[i])\n",
    "            if num_characters >= sent1_word_char_loc[0] or num_characters + word_len >= sent1_word_char_loc[1]:\n",
    "                location_of_word = (i, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            elif num_characters > sent1_word_char_loc[1]:\n",
    "                location_of_word = (i - 1, -1)\n",
    "                word1_not_found = False\n",
    "            else:\n",
    "                num_characters += word_len + 1 \n",
    "                i += 1\n",
    "                \n",
    "        #locate word two\n",
    "        num_characters = 0\n",
    "        \n",
    "        while word2_not_found and j < len(sent2_split):\n",
    "            word_len = len(sent2_split[j])\n",
    "            if num_characters >= sent2_word_char_loc[0] or num_characters + word_len >= sent2_word_char_loc[1]:\n",
    "                location_of_word = (i, j)\n",
    "                word2_not_found = False\n",
    "            elif num_characters > sent2_word_char_loc[1]:\n",
    "                location_of_word = (i, j - 1)\n",
    "                word2_not_found = False\n",
    "            else:\n",
    "                num_characters += word_len + 1\n",
    "                j += 1\n",
    "                \n",
    "        # Now to find the word in the tokenized sentences\n",
    "        word1 = sent1_split[location_of_word[0]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation (See https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)\n",
    "        word2 = sent2_split[location_of_word[1]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation\n",
    "        # print('word1:', word1) # 추가함\n",
    "        # print('word2:', word2) # 추가함\n",
    "        token_word_locs = find_words_in_tokenized_sentences([word1, word2], wic_encoded[-1])\n",
    "        # print('wic_encoded[-1]:', wic_encoded[-1]) # 추가함\n",
    "        # print('token_word_locs:', token_word_locs) # 추가함\n",
    "        # print() # 추가함\n",
    "        wic_word_locs.append(token_word_locs)\n",
    "        \n",
    "        # Get the label if we expect it to be there\n",
    "        if training:\n",
    "            if example['label']:\n",
    "                wic_labels.append(1)\n",
    "            else:\n",
    "                wic_labels.append(0)\n",
    "                \n",
    "    # Pad the sequences and find the encoded word location in the combined input\n",
    "    max_len = np.array([len(ex) for ex in wic_encoded]).max()\n",
    "    wic_padded = {\"input_ids\" : [], \"attention_mask\" : [], \"token_type_ids\" : [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : wic_indexes}\n",
    "    for i in range(0, len(wic_encoded)):\n",
    "        enc_sentence = wic_encoded[i]\n",
    "        location_of_word = wic_word_locs[i]\n",
    "        # Pad the sequences\n",
    "        ex_len = len(enc_sentence)\n",
    "        padded_sentence = enc_sentence.copy()\n",
    "        padded_sentence.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"input_ids\"].append(padded_sentence)\n",
    "        padded_mask = [1] * ex_len\n",
    "        padded_mask.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"attention_mask\"].append(padded_mask)\n",
    "        # Create the vector to get back the words\n",
    "        token_word_locs = wic_word_locs[i]\n",
    "        first_word_loc = []\n",
    "        second_word_loc = []\n",
    "        len_first_word = token_word_locs[0][1] - token_word_locs[0][0] + 1\n",
    "        len_second_word = token_word_locs[1][1] - token_word_locs[1][0] + 1\n",
    "        for j in range(0, max_len):\n",
    "            if j >= token_word_locs[0][0] and j <= token_word_locs[0][1]:\n",
    "                #Part of the first word\n",
    "                first_word_loc.append(1.0 / len_first_word)\n",
    "            else:\n",
    "                first_word_loc.append(0.0)\n",
    "            if j >= token_word_locs[1][0] and j <= token_word_locs[1][1]:\n",
    "                #Part of the second word\n",
    "                second_word_loc.append(1.0 / len_second_word)\n",
    "            else:\n",
    "                second_word_loc.append(0.0)\n",
    "        #We want to append a [1, max_len] vector instead of a [max_len] vector so wrap in an array\n",
    "        wic_padded[\"word1_locs\"].append([first_word_loc])\n",
    "        wic_padded[\"word2_locs\"].append([second_word_loc])\n",
    "        #token_type_ids is a mask that tells where the first and second sentences are\n",
    "        token_type_id = []\n",
    "        first_sentence = True\n",
    "        sentence_start = True\n",
    "        for token in padded_sentence:\n",
    "            if first_sentence and sentence_start and token == 0:\n",
    "                #Allows 0 at the start of the first sentence\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and token > 0:\n",
    "                if sentence_start:\n",
    "                    sentence_start = False\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and not sentence_start and token == 0:\n",
    "                first_sentence = False\n",
    "                #Start of second sentence\n",
    "                token_type_id.append(1)\n",
    "            else:\n",
    "                #Second sentence\n",
    "                token_type_id.append(1)\n",
    "        wic_padded[\"token_type_ids\"].append(token_type_id)\n",
    "        \n",
    "    if training:\n",
    "        for_tensor = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"],\"attention_mask\": wic_padded[\"attention_mask\"], \"labels\": wic_labels, \"index\" : wic_padded[\"index\"],\"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    else:\n",
    "        for_tensor = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"], \"attention_mask\": wic_padded[\"attention_mask\"], \"index\" : wic_padded[\"index\"], \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "\n",
    "    return for_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242.125\n"
     ]
    }
   ],
   "source": [
    "#Data processing\n",
    "train_json_objs = parse_file_to_JSON(f'{my_path}/WiC_train.jsonl')\n",
    "raw_train_set = preprocessing(train_json_objs)\n",
    "print(len(raw_train_set[\"labels\"])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This makes our training data set for the training loop\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(raw_train_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"index\"]).to(device)\n",
    ")\n",
    "\n",
    "#This makes the sampler and data loader for our training loop\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads the jsonl files and make json using the helper functions\n",
    "test_json_objs = parse_file_to_JSON(f'{my_path}/WiC_test.jsonl')\n",
    "valid_json_objs = parse_file_to_JSON(f'{my_path}/WiC_val.jsonl')\n",
    "\n",
    "#This does the preprocessing step for our json objects\n",
    "raw_test_set = preprocessing(test_json_objs, training = False)\n",
    "raw_valid_set = preprocessing(valid_json_objs)\n",
    "\n",
    "#These are out test and validation data sets to be used to get our final accuracy and our results\n",
    "#for the test.jsonl file.\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(raw_test_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"index\"]).to(device)\n",
    ")\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(raw_valid_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"index\"]).to(device)\n",
    ")\n",
    "\n",
    "#This makes the sampler and data loader for the end of our program\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in model\n",
    "\n",
    "class WiC_Head(torch.nn.Module):\n",
    "    def __init__(self, specific_model, embedding_size = 768):\n",
    "        \"\"\"\n",
    "        add a linear layer to take the distance between two \n",
    "        \"\"\"\n",
    "        super(WiC_Head, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = specific_model\n",
    "        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n",
    "        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                word1_locs = None, word2_locs = None):\n",
    "        \"\"\"\n",
    "        same parameters as RoBERTa forward adding two tensors for the location of the 2 words to compare them\n",
    "        \"\"\"\n",
    "        batch_size = word1_locs.shape[0]\n",
    "        # get the embeddings (numerical representation)\n",
    "        embs = self.embedder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state # 수정함\n",
    "        \n",
    "        # The words from the sentences\n",
    "        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n",
    "        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n",
    "        \n",
    "        # seeing how different are the words by substracting the numbers that represent the words\n",
    "        diff = word1s - word2s\n",
    "        \n",
    "        # Calculate outputs using activation\n",
    "        layer1_results = self.activation(self.linear_diff(diff))\n",
    "        logits = self.softmax(self.linear_seperator(layer1_results))\n",
    "        outputs = logits\n",
    "        \n",
    "        # Calculate prediction label\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model to be used\n",
    "class_model = WiC_Head(model, embedding_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the accuracy of our model\n",
    "def flat_accuracy(preds, labels, return_predict_correctness = False):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    if return_predict_correctness:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat\n",
    "    else:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmserver/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\tLoss: 0.6739685773358914; Accuracy: 0.6026234567901234\n",
      "Validation:\n",
      "\tLoss: 0.5871734280843992; Accuracy: 0.7498793436293436\n",
      "\n",
      "Training epoch #2\n",
      "Training:\n",
      "\tLoss: 0.4861577852272693; Accuracy: 0.8320473251028807\n",
      "Validation:\n",
      "\tLoss: 0.4491253454942961; Accuracy: 0.8629343629343629\n",
      "\n",
      "Training epoch #3\n",
      "Training:\n",
      "\tLoss: 0.41121826520181975; Accuracy: 0.9035493827160493\n",
      "Validation:\n",
      "\tLoss: 0.4380272659095558; Accuracy: 0.8722249034749036\n",
      "\n",
      "Training epoch #4\n",
      "Training:\n",
      "\tLoss: 0.38066091630684495; Accuracy: 0.934156378600823\n",
      "Validation:\n",
      "\tLoss: 0.42962821834796183; Accuracy: 0.8806708494208495\n",
      "\n",
      "Training epoch #5\n",
      "Training:\n",
      "\tLoss: 0.3711070517698924; Accuracy: 0.9422582304526749\n",
      "Validation:\n",
      "\tLoss: 0.42425413389463684; Accuracy: 0.885738416988417\n",
      "\n",
      "Training epoch #6\n",
      "Training:\n",
      "\tLoss: 0.36146244983123654; Accuracy: 0.9528034979423868\n",
      "Validation:\n",
      "\tLoss: 0.4345482233408335; Accuracy: 0.8741554054054054\n",
      "\n",
      "Training epoch #7\n",
      "Training:\n",
      "\tLoss: 0.35111877260875307; Accuracy: 0.9638631687242798\n",
      "Validation:\n",
      "\tLoss: 0.42077868290849635; Accuracy: 0.8899613899613901\n",
      "\n",
      "Training epoch #8\n",
      "Training:\n",
      "\tLoss: 0.3503090046315527; Accuracy: 0.9628343621399177\n",
      "Validation:\n",
      "\tLoss: 0.4498590891425674; Accuracy: 0.8595559845559845\n",
      "\n",
      "Training epoch #9\n",
      "Training:\n",
      "\tLoss: 0.3437994129863786; Accuracy: 0.970164609053498\n",
      "Validation:\n",
      "\tLoss: 0.42625595830582286; Accuracy: 0.8882722007722008\n",
      "\n",
      "Training epoch #10\n",
      "Training:\n",
      "\tLoss: 0.34432749738418517; Accuracy: 0.9690072016460906\n",
      "Validation:\n",
      "\tLoss: 0.4216548107765816; Accuracy: 0.8918918918918919\n",
      "\n",
      "Training epoch #11\n",
      "Training:\n",
      "\tLoss: 0.34160463714305267; Accuracy: 0.9715792181069959\n",
      "Validation:\n",
      "\tLoss: 0.4030133977129653; Accuracy: 0.9096283783783784\n",
      "\n",
      "Training epoch #12\n",
      "Training:\n",
      "\tLoss: 0.3408658524109012; Accuracy: 0.9733796296296297\n",
      "Validation:\n",
      "\tLoss: 0.4225444487623266; Accuracy: 0.8918918918918919\n",
      "\n",
      "Training epoch #13\n",
      "Training:\n",
      "\tLoss: 0.3402648693249549; Accuracy: 0.972608024691358\n",
      "Validation:\n",
      "\tLoss: 0.4090669590073663; Accuracy: 0.8986486486486487\n",
      "\n",
      "Training epoch #14\n",
      "Training:\n",
      "\tLoss: 0.3377232432610704; Accuracy: 0.975951646090535\n",
      "Validation:\n",
      "\tLoss: 0.41811194774266836; Accuracy: 0.8927364864864865\n",
      "\n",
      "Training epoch #15\n",
      "Training:\n",
      "\tLoss: 0.33632549950124796; Accuracy: 0.977366255144033\n",
      "Validation:\n",
      "\tLoss: 0.4136871606916995; Accuracy: 0.8969594594594594\n",
      "\n",
      "Training epoch #16\n",
      "Training:\n",
      "\tLoss: 0.3345710478201815; Accuracy: 0.9786522633744856\n",
      "Validation:\n",
      "\tLoss: 0.4097225086109058; Accuracy: 0.9028716216216216\n",
      "\n",
      "Training epoch #17\n",
      "Training:\n",
      "\tLoss: 0.3322888296327473; Accuracy: 0.9810956790123457\n",
      "Validation:\n",
      "\tLoss: 0.4012271584691228; Accuracy: 0.9096283783783784\n",
      "\n",
      "Training epoch #18\n",
      "Training:\n",
      "\tLoss: 0.3329587302826069; Accuracy: 0.9801954732510288\n",
      "Validation:\n",
      "\tLoss: 0.4123125905926163; Accuracy: 0.9011824324324325\n",
      "\n",
      "Training epoch #19\n",
      "Training:\n",
      "\tLoss: 0.33296766693209423; Accuracy: 0.9800668724279835\n",
      "Validation:\n",
      "\tLoss: 0.414708179396552; Accuracy: 0.8978040540540541\n",
      "\n",
      "Training epoch #20\n",
      "Training:\n",
      "\tLoss: 0.33219268255763584; Accuracy: 0.981224279835391\n",
      "Validation:\n",
      "\tLoss: 0.4341988579646961; Accuracy: 0.8766891891891891\n",
      "\n",
      "Training epoch #21\n",
      "Training:\n",
      "\tLoss: 0.3325879979771351; Accuracy: 0.9804526748971193\n",
      "Validation:\n",
      "\tLoss: 0.41611625777708516; Accuracy: 0.8961148648648649\n",
      "\n",
      "Training epoch #22\n",
      "Training:\n",
      "\tLoss: 0.3308248486783769; Accuracy: 0.9822530864197531\n",
      "Validation:\n",
      "\tLoss: 0.41505286500260635; Accuracy: 0.8978040540540541\n",
      "\n",
      "Training epoch #23\n",
      "Training:\n",
      "\tLoss: 0.3292637566725413; Accuracy: 0.9835390946502057\n",
      "Validation:\n",
      "\tLoss: 0.41048047027072393; Accuracy: 0.9011824324324325\n",
      "\n",
      "Training epoch #24\n",
      "Training:\n",
      "\tLoss: 0.3313616723925979; Accuracy: 0.9821244855967078\n",
      "Validation:\n",
      "\tLoss: 0.42012324607050094; Accuracy: 0.8902027027027027\n",
      "\n",
      "Training epoch #25\n",
      "Training:\n",
      "\tLoss: 0.3322773773238492; Accuracy: 0.9803240740740741\n",
      "Validation:\n",
      "\tLoss: 0.41194386337254496; Accuracy: 0.902027027027027\n",
      "\n",
      "Training epoch #26\n",
      "Training:\n",
      "\tLoss: 0.32944795943091437; Accuracy: 0.9836676954732511\n",
      "Validation:\n",
      "\tLoss: 0.4182625968714018; Accuracy: 0.8918918918918919\n",
      "\n",
      "Training epoch #27\n",
      "Training:\n",
      "\tLoss: 0.3298870899059154; Accuracy: 0.9835390946502057\n",
      "Validation:\n",
      "\tLoss: 0.41833658073399516; Accuracy: 0.893581081081081\n",
      "\n",
      "Training epoch #28\n",
      "Training:\n",
      "\tLoss: 0.3293441754800302; Accuracy: 0.9839248971193416\n",
      "Validation:\n",
      "\tLoss: 0.41594553799242584; Accuracy: 0.8961148648648649\n",
      "\n",
      "Best accuracy (0.9096283783783784) obtained at epoch #17.\n"
     ]
    }
   ],
   "source": [
    "#Accuracy desired\n",
    "MIN_ACCURACY = 0.99\n",
    "REACHED_MIN_ACCURACY = False\n",
    "best_weights = class_model.state_dict()\n",
    "max_val_acc = (0, 0) # (accuracy, epoch)\n",
    "#Put the model in the GPU\n",
    "class_model.cuda(1)\n",
    "\n",
    "# Loss Graph\n",
    "train_losses = [None]\n",
    "val_losses = [None]\n",
    "\n",
    "#Optimizer: changing the weights to make the moder optimal\n",
    "param_optimizer = list(class_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "#Optimizer comes from hugging bert models\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "\n",
    "#Storing accuracy and loss\n",
    "fit_history = {\"loss\": [],  \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "epoch_number = 0\n",
    "epoch_since_max = 0\n",
    "continue_learning = True\n",
    "\n",
    "#This loop goes through the training process for each of the epochs\n",
    "while epoch_number < EPOCHS and continue_learning:\n",
    "    epoch_number += 1\n",
    "    print(f\"Training epoch #{epoch_number}\")\n",
    "    #Tracking variables\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    #Set the model to training mode so we can train it\n",
    "    class_model.train()\n",
    "    #Set the final weights\n",
    "    class_model.embedder.requires_grad_ = False\n",
    "    \n",
    "    #This for loop goes through each of the batches in the epochs for training \n",
    "    #This loop trains each batch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #Use the GPU to train the batch\n",
    "        batch = tuple(t.cuda(1) for t in batch)\n",
    "        #Get the items to be used from the data loader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        #Clear out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Forward training\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2) \n",
    "        #Backward training\n",
    "        loss.backward()\n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        #Update data to the CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        #Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids)\n",
    "        #Append to fit history\n",
    "        fit_history[\"loss\"].append(loss.item()) \n",
    "        fit_history[\"accuracy\"].append(b_accuracy) \n",
    "        #Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        tr_accuracy += b_accuracy\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        #This prints the current batch's loss and accuracy\n",
    "        # if nb_tr_steps%10 == 0:\n",
    "        #     print(\"\\t\\tTraining Batch {}: Loss: {}; Accuracy: {}\".format(nb_tr_steps, loss.item(), b_accuracy))\n",
    "    print(\"Training:\\n\\tLoss: {}; Accuracy: {}\".format(tr_loss/nb_tr_steps, tr_accuracy/nb_tr_steps))\n",
    "    train_losses.append(tr_loss/nb_tr_steps)\n",
    "    #Set model to evaluation mode so we can evaluate without training\n",
    "    \n",
    "    class_model.eval()\n",
    "    #Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        #Add batch to GPU\n",
    "        batch = tuple(t.cuda(1) for t in batch)\n",
    "        #Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        #not computing gradients\n",
    "        with torch.no_grad():\n",
    "            #Forward pass, calculate logit predictions\n",
    "            loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "        #Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        #Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids)\n",
    "        #Append to fit history\n",
    "        fit_history[\"val_loss\"].append(loss.item())\n",
    "        fit_history[\"val_accuracy\"].append(b_accuracy) \n",
    "        #Update tracking variables\n",
    "        eval_loss += loss.item()\n",
    "        eval_accuracy += b_accuracy\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "        # if nb_eval_steps%10 == 0:\n",
    "        #     print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "            \n",
    "    #This section of the code is to determine whether we need to keep training or not\n",
    "    #i.e. if we exceed the min acuracy needed we stop training or if we meet the epoch number we specified previously\n",
    "    eval_acc = eval_accuracy/nb_eval_steps\n",
    "    if eval_acc >= max_val_acc[0]:\n",
    "        max_val_acc = (eval_acc, epoch_number)\n",
    "        continue_learning = True\n",
    "        epoch_since_max = 0\n",
    "        #This records the best weights to be added to the trained model\n",
    "        best_weights = copy.deepcopy(class_model.state_dict())\n",
    "        torch.save(class_model.state_dict(), f'./model/model_{model_name}_{str(epoch_number)}.pt') # 모델 저장\n",
    "        #See if we have reached min_accuracy\n",
    "        if eval_acc >= MIN_ACCURACY:\n",
    "            REACHED_MIN_ACCURACY = True\n",
    "        #When it has reached min accuracy we want to end the learning process\n",
    "        if REACHED_MIN_ACCURACY:\n",
    "            continue_learning = False # No necessary to continue learning\n",
    "    else:\n",
    "        epoch_since_max += 1\n",
    "        #If the desired accuracy isn't met, then we stop it with the patience value\n",
    "        if epoch_since_max > PATIENCE:\n",
    "            continue_learning = False\n",
    "    print(\"Validation:\\n\\tLoss: {}; Accuracy: {}\\n\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "    val_losses.append(eval_loss/nb_eval_steps)\n",
    "    \n",
    "print(f\"Best accuracy ({max_val_acc[0]}) obtained at epoch #{max_val_acc[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Trained Model (For Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper to normilize our predictions\n",
    "def normalize(preds):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    return pred_flat == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WiC_Head(\n",
       "  (embedder): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear_diff): Linear(in_features=768, out_features=250, bias=True)\n",
       "  (linear_seperator): Linear(in_features=250, out_features=2, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (activation): ReLU()\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reload the best weights\n",
    "class_model.load_state_dict(torch.load('./model/best/model_KoELECTRA-base_17.pt'))\n",
    "\n",
    "# Put the model in GPU\n",
    "class_model.cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmserver/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tValidation Batch 10: Loss: 0.4491198658943176; Accuracy: 0.84375\n",
      "\t\tValidation Batch 20: Loss: 0.4143008291721344; Accuracy: 0.90625\n",
      "\t\tValidation Batch 30: Loss: 0.4347462058067322; Accuracy: 0.875\n",
      "Validation:\n",
      "\tLoss: 0.4012271584691228; Accuracy: 0.9096283783783784\n"
     ]
    }
   ],
   "source": [
    "validation_predictions_correctness = {}\n",
    "# Validation\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Put model in evaluation mode\n",
    "class_model.eval()\n",
    "\n",
    "#Evaluation loop\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.cuda(1) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    \n",
    "    #Adapted gradient optimizer\n",
    "    with torch.no_grad():\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n",
    "                                    labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "\n",
    "    #Use CPU for accuracy calcs\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.cpu().numpy()\n",
    "    b_accuracy, b_pred_correctness = flat_accuracy(logits, label_ids, return_predict_correctness = True)\n",
    "    indexes = b_index.detach().cpu().numpy()\n",
    "\n",
    "    for index, pred in zip(indexes, b_pred_correctness):\n",
    "        validation_predictions_correctness[index] = pred\n",
    "\n",
    "    eval_loss += loss.item()\n",
    "    eval_accuracy += b_accuracy\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "    if nb_eval_steps%10 == 0:\n",
    "        print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "        \n",
    "print(\"Validation:\\n\\tLoss: {}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "validation_predictions_correctness = collections.OrderedDict(sorted(validation_predictions_correctness.items()))\n",
    "# print(validation_predictions_correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서부터 순서대로 실행하면 inference를 할 수 있게 csv->jsonl 변환 코드를 다시 넣고 필요한 변수와 함수들을 다시 정의하였습니다.\n",
    "- 이 ipynb 파일과 같은 디렉토리 안에 ./data/NIKL_SKT_WiC_Train.tsv, NIKL_SKT_WiC_Dev.tsv, NIKL_SKT_WiC_Test.tsv 파일이 있어야 inference를 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 경로에 데이터가 있어야 함!\n",
    "df_train = pd.read_csv('./data/NIKL_SKT_WiC_Train.tsv', sep=\"\\t\")\n",
    "df_val = pd.read_csv('./data/NIKL_SKT_WiC_Dev.tsv', sep=\"\\t\")\n",
    "df_test = pd.read_csv('./data/NIKL_SKT_WiC_Test.tsv', sep=\"\\t\")\n",
    "\n",
    "df_list = [df_train, df_val, df_test]\n",
    "for df in df_list:\n",
    "    df.rename(columns = {'Target':'word', \n",
    "                         'SENTENCE1':'sentence1',\n",
    "                         'SENTENCE2':'sentence2',\n",
    "                         'ANSWER':'label',\n",
    "                         'start_s1':'start1',\n",
    "                         'start_s2':'start2',\n",
    "                         'end_s1':'end1',\n",
    "                         'end_s2':'end2'}, inplace = True)\n",
    "    \n",
    "# jsonl 파일로 저장\n",
    "df_train.to_json('./data/WiC_train.jsonl', orient='records', lines=True, force_ascii = False)\n",
    "df_val.to_json('./data/WiC_val.jsonl', orient='records', lines=True, force_ascii = False)\n",
    "df_test.to_json('./data/WiC_test.jsonl', orient='records', lines=True, force_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(preds):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    return pred_flat == 1\n",
    "\n",
    "#Testing the accuracy of our model\n",
    "def flat_accuracy(preds, labels, return_predict_correctness = False):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    if return_predict_correctness:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat, pred_flat # 추가함\n",
    "    else:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "        \n",
    "class WiC_Head(torch.nn.Module):\n",
    "    def __init__(self, specific_model, embedding_size = 768):\n",
    "        \"\"\"\n",
    "        add a linear layer to take the distance between two \n",
    "        \"\"\"\n",
    "        super(WiC_Head, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = specific_model\n",
    "        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n",
    "        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                word1_locs = None, word2_locs = None):\n",
    "        \"\"\"\n",
    "        same parameters as RoBERTa forward adding two tensors for the location of the 2 words to compare them\n",
    "        \"\"\"\n",
    "        batch_size = word1_locs.shape[0]\n",
    "        # get the embeddings (numerical representation)\n",
    "        embs = self.embedder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state # 수정함\n",
    "        \n",
    "        # The words from the sentences\n",
    "        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n",
    "        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n",
    "        \n",
    "        # seeing how different are the words by substracting the numbers that represent the words\n",
    "        diff = word1s - word2s\n",
    "        \n",
    "        # Calculate outputs using activation\n",
    "        layer1_results = self.activation(self.linear_diff(diff))\n",
    "        logits = self.softmax(self.linear_seperator(layer1_results))\n",
    "        outputs = logits\n",
    "        \n",
    "        # Calculate prediction label\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        return outputs\n",
    "\n",
    "#Readfile function to take all the objects out of jsonl files\n",
    "def parse_file_to_JSON(filename):\n",
    "    serparated_json_objs = []\n",
    "    \n",
    "    #grab each line and add it as an element in json objs arr\n",
    "    with open(filename, mode = \"r\") as jsonl_file:\n",
    "        for i in jsonl_file:\n",
    "            serparated_json_objs.append(json.loads(i))\n",
    "\n",
    "    return serparated_json_objs\n",
    "\n",
    "#Take a list of words (strings) and a sentence and returns a list\n",
    "#of pairs indicating the tokens' start and end positions in the sentence for each word\n",
    "#Create a function that matches word in tokenized sentence\n",
    "def find_word_in_tokenized_sentence(word,token_ids):\n",
    "    \n",
    "    # decomposedWord = tokenizer.encode(word)\n",
    "    decomposedWord = tokenizer.encode(word)[1:-1] # 수정함\n",
    "    \n",
    "    # print('decomposedWord:', decomposedWord) # 추가함\n",
    "   #Iterate through to find a matching sublist of the token_ids\n",
    "    for i in range(len(token_ids)):\n",
    "        if token_ids[i] == decomposedWord[0] and token_ids[i:i+len(decomposedWord)] == decomposedWord:\n",
    "            return (i,i+len(decomposedWord)-1)\n",
    "    #finalize the output if there is no matching pattern found\n",
    "    # print('returned -1 -1') # 추가함\n",
    "    return (-1,-1)\n",
    "  \n",
    "def find_words_in_tokenized_sentences(wordList,token_ids):\n",
    "    #Create a intList that marks the positions of words\n",
    "    intList = []\n",
    "    #if intList is empty, call the previous function as no matching pattern found\n",
    "    for word in wordList:\n",
    "        if len(intList) == 0:\n",
    "            intList.append(find_word_in_tokenized_sentence(word,token_ids))\n",
    "        else:\n",
    "            afterLastInterval = intList[-1][1]+1\n",
    "            interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:])\n",
    "            actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n",
    "            intList.append(actualPositions)\n",
    "    return intList\n",
    "\n",
    "def preprocessing(json_objects, training = True):\n",
    "    \n",
    "    wic_sentences, wic_encoded, wic_labels, wic_word_locs, wic_indexes = [], [], [] ,[] ,[]\n",
    "    \n",
    "    for index, example in enumerate(json_objects):\n",
    "        \n",
    "        wic_indexes.append(index)\n",
    "        sentence = f\"<s>{example['sentence1']}</s><s>{example['sentence2']}</s>\"\n",
    "        wic_sentences.append(sentence)\n",
    "\n",
    "        wic_encoded.append(tokenizer.encode(sentence))\n",
    "        \n",
    "        # locate word in context\n",
    "        word = example['word']\n",
    "        location_of_word = (-1, -1)\n",
    "        sent1_split = example['sentence1'].split(' ')\n",
    "        sent2_split = example['sentence2'].split(' ')\n",
    "        \n",
    "        # wic indx\n",
    "        sent1_word_char_loc = (example['start1'], example['end1'])\n",
    "        sent2_word_char_loc = (example['start2'], example['end2'])\n",
    "        \n",
    "        num_characters = 0\n",
    "        \n",
    "        i, j = 0, 0\n",
    "        word1_not_found, word2_not_found = True, True\n",
    "        \n",
    "        #locate word one\n",
    "        while word1_not_found and i < len(sent1_split):\n",
    "            word_len = len(sent1_split[i])\n",
    "            if num_characters >= sent1_word_char_loc[0] or num_characters + word_len >= sent1_word_char_loc[1]:\n",
    "                location_of_word = (i, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            elif num_characters > sent1_word_char_loc[1]:\n",
    "                location_of_word = (i - 1, -1)\n",
    "                word1_not_found = False\n",
    "            else:\n",
    "                num_characters += word_len + 1 \n",
    "                i += 1\n",
    "                \n",
    "        #locate word two\n",
    "        num_characters = 0\n",
    "        \n",
    "        while word2_not_found and j < len(sent2_split):\n",
    "            word_len = len(sent2_split[j])\n",
    "            if num_characters >= sent2_word_char_loc[0] or num_characters + word_len >= sent2_word_char_loc[1]:\n",
    "                location_of_word = (i, j)\n",
    "                word2_not_found = False\n",
    "            elif num_characters > sent2_word_char_loc[1]:\n",
    "                location_of_word = (i, j - 1)\n",
    "                word2_not_found = False\n",
    "            else:\n",
    "                num_characters += word_len + 1\n",
    "                j += 1\n",
    "                \n",
    "        # Now to find the word in the tokenized sentences\n",
    "        word1 = sent1_split[location_of_word[0]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation (See https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)\n",
    "        word2 = sent2_split[location_of_word[1]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation\n",
    "        # print('word1:', word1) # 추가함\n",
    "        # print('word2:', word2) # 추가함\n",
    "        token_word_locs = find_words_in_tokenized_sentences([word1, word2], wic_encoded[-1])\n",
    "        # print('wic_encoded[-1]:', wic_encoded[-1]) # 추가함\n",
    "        # print('token_word_locs:', token_word_locs) # 추가함\n",
    "        # print() # 추가함\n",
    "        wic_word_locs.append(token_word_locs)\n",
    "        \n",
    "        # Get the label if we expect it to be there\n",
    "        if training:\n",
    "            if example['label']:\n",
    "                wic_labels.append(1)\n",
    "            else:\n",
    "                wic_labels.append(0)\n",
    "                \n",
    "    # Pad the sequences and find the encoded word location in the combined input\n",
    "    max_len = np.array([len(ex) for ex in wic_encoded]).max()\n",
    "    wic_padded = {\"input_ids\" : [], \"attention_mask\" : [], \"token_type_ids\" : [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : wic_indexes}\n",
    "    for i in range(0, len(wic_encoded)):\n",
    "        enc_sentence = wic_encoded[i]\n",
    "        location_of_word = wic_word_locs[i]\n",
    "        # Pad the sequences\n",
    "        ex_len = len(enc_sentence)\n",
    "        padded_sentence = enc_sentence.copy()\n",
    "        padded_sentence.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"input_ids\"].append(padded_sentence)\n",
    "        padded_mask = [1] * ex_len\n",
    "        padded_mask.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"attention_mask\"].append(padded_mask)\n",
    "        # Create the vector to get back the words\n",
    "        token_word_locs = wic_word_locs[i]\n",
    "        first_word_loc = []\n",
    "        second_word_loc = []\n",
    "        len_first_word = token_word_locs[0][1] - token_word_locs[0][0] + 1\n",
    "        len_second_word = token_word_locs[1][1] - token_word_locs[1][0] + 1\n",
    "        for j in range(0, max_len):\n",
    "            if j >= token_word_locs[0][0] and j <= token_word_locs[0][1]:\n",
    "                #Part of the first word\n",
    "                first_word_loc.append(1.0 / len_first_word)\n",
    "            else:\n",
    "                first_word_loc.append(0.0)\n",
    "            if j >= token_word_locs[1][0] and j <= token_word_locs[1][1]:\n",
    "                #Part of the second word\n",
    "                second_word_loc.append(1.0 / len_second_word)\n",
    "            else:\n",
    "                second_word_loc.append(0.0)\n",
    "        #We want to append a [1, max_len] vector instead of a [max_len] vector so wrap in an array\n",
    "        wic_padded[\"word1_locs\"].append([first_word_loc])\n",
    "        wic_padded[\"word2_locs\"].append([second_word_loc])\n",
    "        #token_type_ids is a mask that tells where the first and second sentences are\n",
    "        token_type_id = []\n",
    "        first_sentence = True\n",
    "        sentence_start = True\n",
    "        for token in padded_sentence:\n",
    "            if first_sentence and sentence_start and token == 0:\n",
    "                #Allows 0 at the start of the first sentence\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and token > 0:\n",
    "                if sentence_start:\n",
    "                    sentence_start = False\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and not sentence_start and token == 0:\n",
    "                first_sentence = False\n",
    "                #Start of second sentence\n",
    "                token_type_id.append(1)\n",
    "            else:\n",
    "                #Second sentence\n",
    "                token_type_id.append(1)\n",
    "        wic_padded[\"token_type_ids\"].append(token_type_id)\n",
    "        \n",
    "    if training:\n",
    "        for_tensor = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"],\"attention_mask\": wic_padded[\"attention_mask\"], \"labels\": wic_labels, \"index\" : wic_padded[\"index\"],\"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    else:\n",
    "        for_tensor = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"], \"attention_mask\": wic_padded[\"attention_mask\"], \"index\" : wic_padded[\"index\"], \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "\n",
    "    return for_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 1. monlogg/koelectra-base-v3-discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 모델 - monlogg/koelectra-base-v3-discriminator\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "model_name = 'KoELECTRA-base-v3-dis'\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "class_model = WiC_Head(model, embedding_size = 768)\n",
    "class_model.load_state_dict(torch.load('./saved_model/동형이의어_KoELECTRA-base-v3-dis.pt'))\n",
    "\n",
    "# Put the model in GPU\n",
    "# class_model.cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads the jsonl files and make json using the helper functions\n",
    "test_json_objs = parse_file_to_JSON('./data/WiC_test.jsonl')\n",
    "valid_json_objs = parse_file_to_JSON('./data/WiC_val.jsonl')\n",
    "\n",
    "#This does the preprocessing step for our json objects\n",
    "raw_test_set = preprocessing(test_json_objs, training = False)\n",
    "raw_valid_set = preprocessing(valid_json_objs)\n",
    "\n",
    "#These are out test and validation data sets to be used to get our final accuracy and our results\n",
    "#for the test.jsonl file.\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(raw_test_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"index\"]).to(device)\n",
    ")\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(raw_valid_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"index\"]).to(device)\n",
    ")\n",
    "\n",
    "#This makes the sampler and data loader for the end of our program\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmserver/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tValidation Batch 10: Loss: 0.4646112322807312; Accuracy: 0.84375\n",
      "\t\tValidation Batch 20: Loss: 0.3987000584602356; Accuracy: 0.90625\n",
      "\t\tValidation Batch 30: Loss: 0.4068596363067627; Accuracy: 0.90625\n",
      "Validation:\n",
      "\tLoss: 0.40866498528300105; Accuracy: 0.9045608108108109\n"
     ]
    }
   ],
   "source": [
    "validation_predictions_correctness = {}\n",
    "# Validation\n",
    "\n",
    "pred_all = np.empty((0))\n",
    "ans_all = np.empty((0))\n",
    "logit_all = np.empty((0,2))\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Put model in evaluation mode\n",
    "class_model.eval()\n",
    "\n",
    "#Evaluation loop\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    # batch = tuple(t.cuda(1) for t in batch) # GPU 사용할 때\n",
    "    batch = tuple(t for t in batch) # CPU 사용할 때\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    \n",
    "    #Adapted gradient optimizer\n",
    "    with torch.no_grad():\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n",
    "                                    labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "\n",
    "    #Use CPU for accuracy calcs\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    logit_all = np.append(logit_all, logits, axis=0)\n",
    "    label_ids = b_labels.cpu().numpy()\n",
    "    b_accuracy, b_pred_correctness, b_pred = flat_accuracy(logits, label_ids, return_predict_correctness = True)\n",
    "\n",
    "    # pred_all.extend(b_pred.tolist())\n",
    "    # ans_all.extend(label_ids.tolist())\n",
    "    \n",
    "    pred_all = np.append(pred_all, b_pred, axis=0)\n",
    "    ans_all = np.append(ans_all, label_ids, axis=0)\n",
    "    \n",
    "    indexes = b_index.detach().cpu().numpy()\n",
    "\n",
    "    for index, pred in zip(indexes, b_pred_correctness):\n",
    "        validation_predictions_correctness[index] = pred\n",
    "\n",
    "    eval_loss += loss.item()\n",
    "    eval_accuracy += b_accuracy\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "    if nb_eval_steps%10 == 0:\n",
    "        print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "        \n",
    "print(\"Validation:\\n\\tLoss: {}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "validation_predictions_correctness = collections.OrderedDict(sorted(validation_predictions_correctness.items()))\n",
    "# print(validation_predictions_correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monlogg/koelectra-base-v3-discriminator 단일 모델 Accuracy: 90.46%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_F</th>\n",
       "      <th>prob_T</th>\n",
       "      <th>prediction</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>0.421832</td>\n",
       "      <td>0.578168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>0.999926</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        prob_F    prob_T  prediction  answer\n",
       "0     0.999983  0.000017         0.0     0.0\n",
       "1     0.999988  0.000012         0.0     0.0\n",
       "2     0.999988  0.000012         0.0     0.0\n",
       "3     0.000025  0.999975         1.0     1.0\n",
       "4     0.000017  0.999983         1.0     1.0\n",
       "...        ...       ...         ...     ...\n",
       "1161  0.999993  0.000007         0.0     0.0\n",
       "1162  0.421832  0.578168         1.0     1.0\n",
       "1163  0.000008  0.999992         1.0     1.0\n",
       "1164  0.999926  0.000074         0.0     0.0\n",
       "1165  0.999998  0.000002         0.0     0.0\n",
       "\n",
       "[1166 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.concatenate((logit_all, np.reshape(pred_all,(pred_all.shape[0],-1)), np.reshape(ans_all,(ans_all.shape[0],-1))), axis=1)\n",
    "df_result1 = pd.DataFrame(result, columns = ['prob_F', 'prob_T', 'prediction', 'answer'])\n",
    "df_result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 2. tunib/electra-ko-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tunib/electra-ko-base were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 모델 - tunib/electra-ko-base\n",
    "model = AutoModel.from_pretrained('tunib/electra-ko-base')\n",
    "model_name = 'KoELECTRA-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained('tunib/electra-ko-base')\n",
    "class_model = WiC_Head(model, embedding_size = 768)\n",
    "class_model.load_state_dict(torch.load('./saved_model/동형이의어_KoELECTRA-base.pt'))\n",
    "\n",
    "# Put the model in GPU\n",
    "# class_model.cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads the jsonl files and make json using the helper functions\n",
    "test_json_objs = parse_file_to_JSON('./data/WiC_test.jsonl')\n",
    "valid_json_objs = parse_file_to_JSON('./data/WiC_val.jsonl')\n",
    "\n",
    "#This does the preprocessing step for our json objects\n",
    "raw_test_set = preprocessing(test_json_objs, training = False)\n",
    "raw_valid_set = preprocessing(valid_json_objs)\n",
    "\n",
    "#These are out test and validation data sets to be used to get our final accuracy and our results\n",
    "#for the test.jsonl file.\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(raw_test_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"index\"]).to(device)\n",
    ")\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(raw_valid_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"index\"]).to(device)\n",
    ")\n",
    "\n",
    "#This makes the sampler and data loader for the end of our program\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmserver/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tValidation Batch 10: Loss: 0.44911959767341614; Accuracy: 0.84375\n",
      "\t\tValidation Batch 20: Loss: 0.4143008291721344; Accuracy: 0.90625\n",
      "\t\tValidation Batch 30: Loss: 0.4347462058067322; Accuracy: 0.875\n",
      "Validation:\n",
      "\tLoss: 0.4012270916152645; Accuracy: 0.9096283783783784\n"
     ]
    }
   ],
   "source": [
    "validation_predictions_correctness = {}\n",
    "# Validation\n",
    "\n",
    "pred_all = np.empty((0))\n",
    "ans_all = np.empty((0))\n",
    "logit_all = np.empty((0,2))\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Put model in evaluation mode\n",
    "class_model.eval()\n",
    "\n",
    "#Evaluation loop\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    # batch = tuple(t.cuda(1) for t in batch) # GPU 사용할 때\n",
    "    batch = tuple(t for t in batch) # CPU 사용할 때\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    \n",
    "    #Adapted gradient optimizer\n",
    "    with torch.no_grad():\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n",
    "                                    labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "\n",
    "    #Use CPU for accuracy calcs\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    logit_all = np.append(logit_all, logits, axis=0)\n",
    "    label_ids = b_labels.cpu().numpy()\n",
    "    b_accuracy, b_pred_correctness, b_pred = flat_accuracy(logits, label_ids, return_predict_correctness = True)\n",
    "\n",
    "    # pred_all.extend(b_pred.tolist())\n",
    "    # ans_all.extend(label_ids.tolist())\n",
    "    \n",
    "    pred_all = np.append(pred_all, b_pred, axis=0)\n",
    "    ans_all = np.append(ans_all, label_ids, axis=0)\n",
    "    \n",
    "    indexes = b_index.detach().cpu().numpy()\n",
    "\n",
    "    for index, pred in zip(indexes, b_pred_correctness):\n",
    "        validation_predictions_correctness[index] = pred\n",
    "\n",
    "    eval_loss += loss.item()\n",
    "    eval_accuracy += b_accuracy\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "    if nb_eval_steps%10 == 0:\n",
    "        print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "        \n",
    "print(\"Validation:\\n\\tLoss: {}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "validation_predictions_correctness = collections.OrderedDict(sorted(validation_predictions_correctness.items()))\n",
    "# print(validation_predictions_correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tunib/electra-ko-base 단일 모델 Accuracy: 90.96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_F</th>\n",
       "      <th>prob_T</th>\n",
       "      <th>prediction</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999890</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999885</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>0.205595</td>\n",
       "      <td>0.794405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        prob_F    prob_T  prediction  answer\n",
       "0     0.999890  0.000110         0.0     0.0\n",
       "1     0.999967  0.000033         0.0     0.0\n",
       "2     0.999885  0.000115         0.0     0.0\n",
       "3     0.000002  0.999997         1.0     1.0\n",
       "4     0.000015  0.999985         1.0     1.0\n",
       "...        ...       ...         ...     ...\n",
       "1161  0.999991  0.000009         0.0     0.0\n",
       "1162  0.205595  0.794405         1.0     1.0\n",
       "1163  0.000002  0.999998         1.0     1.0\n",
       "1164  0.999980  0.000020         0.0     0.0\n",
       "1165  0.999983  0.000017         0.0     0.0\n",
       "\n",
       "[1166 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.concatenate((logit_all, np.reshape(pred_all,(pred_all.shape[0],-1)), np.reshape(ans_all,(ans_all.shape[0],-1))), axis=1)\n",
    "df_result2 = pd.DataFrame(result, columns = ['prob_F', 'prob_T', 'prediction', 'answer'])\n",
    "df_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble (Soft Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 label\n",
    "answer = df_result1['answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1 = df_result1['prob_F']\n",
    "prob2 = df_result2['prob_F']\n",
    "\n",
    "prob = (prob1+prob2)/2\n",
    "prob = prob.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9228130360205832\n"
     ]
    }
   ],
   "source": [
    "final_pred = []\n",
    "for i in prob:\n",
    "    if i >= 0.5: \n",
    "        final_pred.append(0)\n",
    "    else: \n",
    "        final_pred.append(1)\n",
    "\n",
    "print(accuracy_score(answer, final_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 Accuracy: 92.28%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 인과관계추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from importlib import import_module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        return\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        :param inputs: predictions\n",
    "        :param target: target labels\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        # target = torch.argmax(target, dim=-1)\n",
    "        loss = self.CE(inputs, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "_criterion_entrypoints = {\n",
    "    'cross_entropy': CrossEntropy,\n",
    "}\n",
    "\n",
    "def criterion_entrypoint(criterion_name):\n",
    "    return _criterion_entrypoints[criterion_name]\n",
    "\n",
    "def is_criterion(criterion_name):\n",
    "    return criterion_name in _criterion_entrypoints\n",
    "\n",
    "def create_criterion(criterion_name, **kwargs):\n",
    "    if is_criterion(criterion_name):\n",
    "        create_fn = criterion_entrypoint(criterion_name)\n",
    "        criterion = create_fn(**kwargs)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown loss (%s)' % criterion_name)\n",
    "    return criterion\n",
    "\n",
    "\n",
    "\n",
    "# Dataset 구성.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', names=['ID', 'sentence', 'question', '1', '2','answer'], header=0)\n",
    "    dataset[\"label\"] = dataset[\"answer\"].astype(int) - 1\n",
    "\n",
    "    new_sentence1_1 = []\n",
    "    new_sentence1_2 = []\n",
    "    new_sentence2_1 = []\n",
    "    new_sentence2_2 = []\n",
    "    for i in range(len(dataset)):\n",
    "        s = dataset.iloc[i]['sentence']\n",
    "        q = dataset.iloc[i]['question']\n",
    "        s1 = dataset.iloc[i]['1']\n",
    "        s2 = dataset.iloc[i]['2']\n",
    "        lb = dataset.iloc[i]['label']\n",
    "        if q == \"결과\":\n",
    "            new_sentence1_1.append(\"[결과]\" + s)\n",
    "            # new_sentence1_1.append(s)\n",
    "            new_sentence1_2.append(s1)\n",
    "            new_sentence2_1.append(\"[결과]\" + s)\n",
    "              # new_sentence2_1.append(s)\n",
    "            new_sentence2_2.append(s2)\n",
    "\n",
    "        else:\n",
    "            new_sentence1_1.append(\"[원인]\" + s1)\n",
    "            # new_sentence1_1.append(s1)\n",
    "            new_sentence1_2.append(s)\n",
    "            new_sentence2_1.append(\"[원인]\" + s2)\n",
    "            # new_sentence2_1.append(s2)\n",
    "            new_sentence2_2.append(s)\n",
    "\n",
    "    dataset[\"new_sentence1_1\"] = new_sentence1_1\n",
    "    dataset[\"new_sentence1_2\"] = new_sentence1_2\n",
    "    dataset[\"new_sentence2_1\"] = new_sentence2_1\n",
    "    dataset[\"new_sentence2_2\"] = new_sentence2_2\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence1_1 = dataset['new_sentence1_1'].tolist()\n",
    "    sentence1_2 = dataset['new_sentence1_2'].tolist()\n",
    "    sentence2_1 = dataset[\"new_sentence2_1\"].tolist()\n",
    "    sentence2_2 = dataset[\"new_sentence2_2\"].tolist()\n",
    "\n",
    "    tokenized_sentences = tokenizer(\n",
    "          sentence1_1,\n",
    "          sentence1_2,\n",
    "          return_tensors=\"pt\",\n",
    "          padding=True,\n",
    "          truncation=True,\n",
    "          max_length=150,\n",
    "          add_special_tokens=True,\n",
    "          return_token_type_ids = True\n",
    "          )\n",
    "    tokenized_sentences2 = tokenizer(\n",
    "          sentence2_1,\n",
    "          sentence2_2,\n",
    "          return_tensors=\"pt\",\n",
    "          padding=True,\n",
    "          truncation=True,\n",
    "          max_length=150,\n",
    "          add_special_tokens=True,\n",
    "          return_token_type_ids = True\n",
    "          )\n",
    "    for key, value in tokenized_sentences2.items():\n",
    "        tokenized_sentences[key+\"2\"] = value\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "# seed 고정 \n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertPreTrainedModel, ElectraModel, ElectraPreTrainedModel, XLMRobertaModel, BartModel, BartPretrainedModel, T5Model, RobertaModel \n",
    "from transformers import MBartModel, MBartConfig\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class PoolingHead(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        inner_dim: int,\n",
    "        pooler_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Electra_BoolQ(ElectraPreTrainedModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(Electra_BoolQ, self).__init__(config)\n",
    "\n",
    "        #self.num_labels = config.num_labels\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator', config=config)\n",
    "        self.pooling = PoolingHead(input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1)\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels -1)\n",
    "        #self.sparse = Sparsemax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, input_ids2=None, attention_mask2=None, token_type_ids2=None, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs2 = self.model(\n",
    "            input_ids2, attention_mask=attention_mask2, token_type_ids=token_type_ids2\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output2 = outputs2[0]\n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "        pooled_output2 = outputs2[0][:, 0, :]\n",
    "\n",
    "        sentence_representation = torch.cat([pooled_output, pooled_output2], dim=1)\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output2 = self.pooling(pooled_output2)\n",
    "        \n",
    "        logits1 = self.qa_classifier(pooled_output)\n",
    "        logits2 = self.qa_classifier(pooled_output2)\n",
    "\n",
    "        logits = torch.cat([logits1, logits2], dim=1)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class CustomPreTrainModel(RobertaModel):\n",
    "    def __init__(self, config, model):\n",
    "        super(CustomPreTrainModel, self).__init__(config)\n",
    "        self.model = model\n",
    "        self.num_labels = 2\n",
    "        self.qa_classifier_final = nn.Linear(self.num_labels*2, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, input_ids2=None, attention_mask2=None, token_type_ids2=None, labels=None):\n",
    "        out1 = self.model(input_ids, attention_mask=attention_mask)[0]   # B, C   (여기서 C는 2라서 concat후 다시 2차원 변환 필요)\n",
    "        out2 = self.model(input_ids2, attention_mask=attention_mask2)[0]\n",
    "\n",
    "        out_cat = torch.cat([out1, out2], dim=1)   # B, C*2\n",
    "        logits = self.qa_classifier_final(out_cat)\n",
    "\n",
    "        outputs = (logits,) + (0,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load, Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 구성.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', names=['ID', 'sentence', 'question', '1', '2','answer'], header=0)\n",
    "    dataset[\"label\"] = dataset[\"answer\"].astype(int) - 1\n",
    "\n",
    "    new_sentence1_1 = []\n",
    "    new_sentence1_2 = []\n",
    "    new_sentence2_1 = []\n",
    "    new_sentence2_2 = []\n",
    "    for i in range(len(dataset)):\n",
    "        s = dataset.iloc[i]['sentence']\n",
    "        q = dataset.iloc[i]['question']\n",
    "        s1 = dataset.iloc[i]['1']\n",
    "        s2 = dataset.iloc[i]['2']\n",
    "        lb = dataset.iloc[i]['label']\n",
    "        if q == \"결과\":\n",
    "            new_sentence1_1.append(\"[결과]\" + s)\n",
    "            # new_sentence1_1.append(s)\n",
    "            new_sentence1_2.append(s1)\n",
    "            new_sentence2_1.append(\"[결과]\" + s)\n",
    "              # new_sentence2_1.append(s)\n",
    "            new_sentence2_2.append(s2)\n",
    "\n",
    "        else:\n",
    "            new_sentence1_1.append(\"[원인]\" + s1)\n",
    "            # new_sentence1_1.append(s1)\n",
    "            new_sentence1_2.append(s)\n",
    "            new_sentence2_1.append(\"[원인]\" + s2)\n",
    "            # new_sentence2_1.append(s2)\n",
    "            new_sentence2_2.append(s)\n",
    "\n",
    "    dataset[\"new_sentence1_1\"] = new_sentence1_1\n",
    "    dataset[\"new_sentence1_2\"] = new_sentence1_2\n",
    "    dataset[\"new_sentence2_1\"] = new_sentence2_1\n",
    "    dataset[\"new_sentence2_2\"] = new_sentence2_2\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence1_1 = dataset['new_sentence1_1'].tolist()\n",
    "    sentence1_2 = dataset['new_sentence1_2'].tolist()\n",
    "    sentence2_1 = dataset[\"new_sentence2_1\"].tolist()\n",
    "    sentence2_2 = dataset[\"new_sentence2_2\"].tolist()\n",
    "\n",
    "    tokenized_sentences = tokenizer(\n",
    "          sentence1_1,\n",
    "          sentence1_2,\n",
    "          return_tensors=\"pt\",\n",
    "          padding=True,\n",
    "          truncation=True,\n",
    "          max_length=150,\n",
    "          add_special_tokens=True,\n",
    "          return_token_type_ids = True\n",
    "          )\n",
    "    tokenized_sentences2 = tokenizer(\n",
    "          sentence2_1,\n",
    "          sentence2_2,\n",
    "          return_tensors=\"pt\",\n",
    "          padding=True,\n",
    "          truncation=True,\n",
    "          max_length=150,\n",
    "          add_special_tokens=True,\n",
    "          return_token_type_ids = True\n",
    "          )\n",
    "    for key, value in tokenized_sentences2.items():\n",
    "        tokenized_sentences[key+\"2\"] = value\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_type', type=str, default='BertBase')\n",
    "parser.add_argument('--pretrained_model', type=str, default='bert-base-uncased')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=10)\n",
    "parser.add_argument('--freeze_epoch', type=int, default=0)\n",
    "parser.add_argument('--batch_size', type=int, default=4)\n",
    "parser.add_argument('--valid_batch_size', type=int, default=128)\n",
    "parser.add_argument('--val_ratio', type=float, default=0.2, help='ratio for validaton (default: 0.2)')\n",
    "parser.add_argument('--dropout_rate', type=float, default=0.1, help=\"Dropout for fully-connected layers\")\n",
    "\n",
    "parser.add_argument('--criterion', type=str, default='cross_entropy', help='criterion type (default: cross_entropy)')\n",
    "parser.add_argument('--optimizer', type=str, default='AdamW', help='optimizer type (default: AdamW)')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1e-6)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.01)\n",
    "parser.add_argument('--warmup_steps', type=int, default=500)               # number of warmup steps for learning rate scheduler\n",
    "parser.add_argument('--seed' , type=int , default = 42, help='random seed (default: 42)')\n",
    "parser.add_argument('--log_interval', type=int, default=20, help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--kfold', type=int, default=1, help='k-fold currunt step number')\n",
    "\n",
    "parser.add_argument('--name', default='exp', help='model save at {SM_MODEL_DIR}/{name}')\n",
    "parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', './results'))\n",
    "parser.add_argument('--custompretrain', type=str, default=\"\", help='Use custom pretrain : model dir')\n",
    "\n",
    "args = parser.parse_args(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = 15\n",
    "args.pretrained_model = 'monologg/koelectra-base-v3-discriminator'\n",
    "args.lr = 8e-6\n",
    "args.batch_size = 16\n",
    "\n",
    "args.kfold = 1\n",
    "args.batch_size=4\n",
    "args.valid_batch_size=16\n",
    "args.model_type = \"koelectra-base-v3-discriminator\"\n",
    "\n",
    "args.name = f'Train_{args.model_type}_{args.lr}'\n",
    "\n",
    "MODEL_NAME = args.pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(GPU) : True\n"
     ]
    }
   ],
   "source": [
    "seed_everything(args.seed)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device(GPU) : {torch.cuda.is_available()}\")\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data, Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizerƒ\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# load dataset\n",
    "train_dataset = load_data(\"./data/SKT_COPA_Train.tsv\")\n",
    "val_dataset = load_data(\"./data/SKT_COPA_Dev.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "train_label = train_dataset['label'].values\n",
    "val_label = val_dataset['label'].values\n",
    "\n",
    "# tokenizing dataset\n",
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "tokenized_val = tokenized_dataset(val_dataset, tokenizer)\n",
    "\n",
    "# make dataset for pytorch.\n",
    "train_dataset = CustomDataset(tokenized_train, train_label)\n",
    "val_dataset = CustomDataset(tokenized_val, val_label)\n",
    "\n",
    "config_module = getattr(import_module(\"transformers\"), \"ElectraConfig\")\n",
    "model_config = config_module.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 2\n",
    "model_module = eval('Electra_BoolQ')\n",
    "\n",
    "save_dir = args.model_dir+\"3\"\n",
    "model = model_module(config=model_config, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- data_loader\n",
    "train_loader = DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=args.batch_size,\n",
    "      shuffle=True,\n",
    "      drop_last=True,\n",
    "  )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "      val_dataset,\n",
    "      batch_size=args.valid_batch_size,\n",
    "      shuffle=False,\n",
    "      drop_last=False,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if ('cls_fc_layer' not in name) and ('label_classifier' not in name): # classifier layer\n",
    "        param.requires_grad = False\n",
    "\n",
    "  # -- loss & metric\n",
    "criterion = create_criterion(args.criterion)  # default: cross_entropy\n",
    "opt_module = getattr(import_module(\"transformers\"), args.optimizer)\n",
    "optimizer = opt_module(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        eps = 1e-8\n",
    "    )\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=args.warmup_steps, \n",
    "    num_training_steps=len(train_loader) * args.epochs, \n",
    "    last_epoch=- 1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_dir, args):\n",
    "\n",
    "    bs_list = [32,16,8,4,2]\n",
    "\n",
    "    for bs in bs_list:\n",
    "        print(\"batch_size : \"+ str(bs))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(vars(args), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "        best_val_acc = 0\n",
    "        best_val_loss = np.inf\n",
    "        for epoch in range(args.epochs):\n",
    "            # train loop\n",
    "            # unFreeze parameters\n",
    "            if epoch == args.freeze_epoch:\n",
    "                for name, param in model.named_parameters():\n",
    "                    param.requires_grad = True\n",
    "            model.train()\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "            for idx, items in enumerate(train_loader):\n",
    "                item = {key: val for key, val in items.items()}\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outs = model(**item)\n",
    "                loss = criterion(outs[0], item['labels'])\n",
    "\n",
    "                preds = torch.argmax(outs[0], dim=-1)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                loss_value += loss.item()\n",
    "                matches += (preds == item['labels']).sum().item()\n",
    "                if (idx + 1) % args.log_interval == 0:\n",
    "                    train_loss = loss_value / args.log_interval\n",
    "                    train_acc = matches / args.batch_size / args.log_interval\n",
    "                    current_lr = get_lr(optimizer)\n",
    "                    print(\n",
    "                        f\"Epoch[{epoch}/{args.epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                        f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "                    loss_value = 0\n",
    "                    matches = 0\n",
    "\n",
    "            # val loop\n",
    "            with torch.no_grad():\n",
    "                print(\"Calculating validation results...\")\n",
    "                model.eval()\n",
    "                val_loss_items = []\n",
    "                val_acc_items = []\n",
    "                acc_okay = 0\n",
    "                count_all = 0\n",
    "                for idx, items in enumerate(tqdm(val_loader)):\n",
    "                    sleep(0.01)\n",
    "                    item = {key: val for key, val in items.items()}\n",
    "\n",
    "                    outs = model(**item)\n",
    "\n",
    "                    preds = torch.argmax(outs[0], dim=-1)\n",
    "                    loss = criterion(outs[0], item['labels']).item()\n",
    "\n",
    "                    acc_item = (item['labels'] == preds).sum().item()\n",
    "\n",
    "                    val_loss_items.append(loss)\n",
    "                    val_acc_items.append(acc_item)\n",
    "                    acc_okay += acc_item\n",
    "                    count_all += len(preds)\n",
    "\n",
    "                val_loss = np.sum(val_loss_items) / len(val_loss_items)\n",
    "                val_acc = acc_okay / count_all\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    print(f\"New best model for val acc : {val_acc:4.2%}! saving the best model..\")\n",
    "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                    model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "                    torch.save(args, os.path.join(f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "                    best_val_acc = val_acc\n",
    "                    bs_best_dict2[bs][0] = best_val_acc\n",
    "                    bs_best_dict2[bs][1] = val_loss\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                print(f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.4}|| \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.4}\")\n",
    "\n",
    "\n",
    "                s = f'Time elapsed: {(time.time() - start_time)/60: .2f} min'\n",
    "                print(s)\n",
    "                print()\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenized_sent):\n",
    "    dataloader = DataLoader(tokenized_sent, batch_size=8, shuffle=False)\n",
    "    model.eval()\n",
    "    results = []\n",
    "    preds = []\n",
    "  \n",
    "    for i, items in enumerate(dataloader):\n",
    "        item = {key: val for key, val in items.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**item)\n",
    "        logits = outputs[0]\n",
    "        m = nn.Softmax(dim=1)\n",
    "        logits = m(logits)\n",
    "        logits = logits.detach().cpu().numpy()   # (Batch_size, 5)  5개의 클래스 확률형태\n",
    "        pred = logits[:,1]\n",
    "        result = np.argmax(logits, axis=-1)\n",
    "        results += result.tolist()\n",
    "        preds += pred.tolist()\n",
    "\n",
    "    return np.array(results).flatten(), np.array(preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.894\n"
     ]
    }
   ],
   "source": [
    "TOK_NAME = args.pretrained_model\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOK_NAME)\n",
    "\n",
    "# load my model \n",
    "model_module = eval(\"Electra_BoolQ\")\n",
    "\n",
    "dataset = load_data(\"./data/SKT_COPA_Dev.tsv\")\n",
    "test_label = [0] * len(dataset)\n",
    "tokenized_test = tokenized_dataset(dataset, tokenizer)\n",
    "test_dataset = CustomDataset(tokenized_test, test_label)\n",
    "\n",
    "model = model_module.from_pretrained(\"./saved_model/Task_3_Model\", args=args)  \n",
    "model.parameters\n",
    "model.eval()\n",
    "pred_answer, preds = inference(model, test_dataset)\n",
    "\n",
    "print(\"Validation Accuracy : \" + str(accuracy_score(dataset[\"label\"],pred_answer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 판정의문문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "#Custom dataset for the experiment\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "#Preprocessing the data for QA experiment\n",
    "#Translate special symbols and punctuation marks into Korean.\n",
    "#using lower function for effective experiment\n",
    "\n",
    "def pre_process(st):\n",
    "    st = re.sub('\\(.*\\)|\\s-\\s.*', '', st)\n",
    "    st = re.sub('\\[.*\\]|\\s-\\s.*', '', st)\n",
    "    st = st.lower()\n",
    "\n",
    "    st = re.sub('[”“]', '\\\"', st)\n",
    "    st = re.sub('[’‘]', '\\'', st)\n",
    "    st = re.sub('[≫〉》＞』」]', '>', st)\n",
    "    st = re.sub('[《「『〈≪＜]','<',st)\n",
    "    st = re.sub('[−–—]', '−', st)\n",
    "    st = re.sub('[･•・‧]','·', st)\n",
    "    \n",
    "    st = st.replace('／', '/')\n",
    "    st = st.replace('℃', '도')\n",
    "    st = st.replace('→', '에서')\n",
    "    st = st.replace('!', '')\n",
    "    st = st.replace('，', ',')\n",
    "    st = st.replace('㎢', 'km')\n",
    "    st = st.replace('∼', '~')\n",
    "    st = st.replace('㎜', 'mm')\n",
    "    st = st.replace('×', '곱하기')\n",
    "    st = st.replace('=', '는')\n",
    "    st = st.replace('®', '')\n",
    "    st = st.replace('㎖', 'ml')\n",
    "    st = st.replace('ℓ', 'l')\n",
    "    st = st.replace('˚C', '도')\n",
    "    st = st.replace('˚', '도')\n",
    "    st = st.replace('°C', '도')\n",
    "    st = st.replace('°', '도')\n",
    "    st = st.replace('＋', '+')\n",
    "    st = st.replace('*', '')\n",
    "    st = st.replace(';', '.')\n",
    "    return st\n",
    "\n",
    "\n",
    "#Load data function to bring in data\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', names=['ID', 'text', 'question', 'answer'], header=0)\n",
    "    dataset[\"label\"] = dataset[\"answer\"].astype(int)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#Use encoder and decoder to tokenize the dataset\n",
    "def tokenized_dataset(dataset, tokenizer, arch=\"encoder\"):\n",
    "    sentence = dataset['text'].tolist()\n",
    "    sentence_d = dataset['question'].tolist()\n",
    "\n",
    "    if arch==\"encoder\":        \n",
    "        tokenized_sentences = tokenizer(\n",
    "            sentence,\n",
    "            sentence_d,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=350,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids = True\n",
    "            )\n",
    "        \n",
    "        \n",
    "    elif arch == \"encoder-decoder\":\n",
    "        tokenized_sentences = tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=350,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids = False\n",
    "            )\n",
    "        tokenized_sentences_d = tokenizer(\n",
    "            sentence_d,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=350,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids = False\n",
    "            )\n",
    "        \n",
    "        for key, value in tokenized_sentences_d.items():\n",
    "            tokenized_sentences[key+\"_d\"] = value\n",
    "\n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaModel\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#We have to use this function when we use XLMRobertaModel\n",
    "class get_similarity(XLMRobertaModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(get_similarity, self).__init__(config)\n",
    "        self.xlmroberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\")\n",
    "        self.num_labels = config.num_labels\n",
    "  \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        outputs = self.xlmroberta(\n",
    "          input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "        print(\"forward outputs: \", outputs)\n",
    "\n",
    "\n",
    "#This function is ued to check the architecture\n",
    "def check_arch(model_type):\n",
    "    archs = {\n",
    "      \"encoder\" : [\"Bert\", \"Electra\", \"XLMRoberta\", \"Electra_BoolQ\", \"Roberta\"],\n",
    "      \"encoder-decoder\" : [\"T5\", \"Bart\", \"Bart_BoolQ\"]\n",
    "    }\n",
    "    for arch in archs:\n",
    "        if model_type in archs[arch]:\n",
    "            return arch\n",
    "    raise ValueError(f\"Model [{model_type}] didn't defined archtecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing modules\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#Compute the cross entropy loss between input and target\n",
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        return\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        loss = self.CE(inputs, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "_criterion_entrypoints = {\n",
    "    'cross_entropy': CrossEntropy,\n",
    "}\n",
    "\n",
    "#?\n",
    "def criterion_entrypoint(criterion_name):\n",
    "    return _criterion_entrypoints[criterion_name]\n",
    "\n",
    "def is_criterion(criterion_name):\n",
    "    return criterion_name in _criterion_entrypoints\n",
    "\n",
    "def create_criterion(criterion_name, **kwargs):\n",
    "    if is_criterion(criterion_name):\n",
    "        create_fn = criterion_entrypoint(criterion_name)\n",
    "        criterion = create_fn(**kwargs)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown loss (%s)' % criterion_name)\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertPreTrainedModel, ElectraModel, ElectraPreTrainedModel, XLMRobertaModel, BartModel, BartPretrainedModel, T5Model, RobertaModel \n",
    "from transformers import MBartModel, MBartConfig\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class PoolingHead(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        inner_dim: int,\n",
    "        pooler_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, inner_dim)\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        return hidden_states\n",
    "        \n",
    "#RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
    "#Carefully measure the impact of many key hyperparameters and training data size\n",
    "#Achieve state-of-the-art results on GLUE, RACE and SQuAD\n",
    "class Roberta(RobertaModel):\n",
    "    def __init__(self, config, args):\n",
    "        super(Roberta, self).__init__(config)\n",
    "        self.roberta = RobertaModel.from_pretrained(\"klue/roberta-large\", config=config)  # Load pretrained Electra\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pooling = PoolingHead(input_dim=config.hidden_size,\n",
    "            inner_dim=config.hidden_size,\n",
    "            pooler_dropout=0.1)\n",
    "        self.qa_classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.roberta(\n",
    "            input_ids, attention_mask=attention_mask\n",
    "        )  \n",
    "        pooled_output = outputs[0][:, 0, :]  # [CLS]\n",
    "\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        \n",
    "        \n",
    "        logits = self.qa_classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  \n",
    "\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modeuls\n",
    "import pickle as pickle\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import MBartModel, MBartConfig\n",
    "import transformers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "#Fix seed \n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "#Metric computing function to compute accuracy\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "    }\n",
    "\n",
    "\n",
    "def increment_output_dir(output_path, exist_ok=False):\n",
    "    path = Path(output_path)\n",
    "    if (path.exists() and exist_ok) or (not path.exists()):\n",
    "        return str(path)\n",
    "    else:\n",
    "        dirs = glob.glob(f\"{path}*\")\n",
    "        matches = [re.search(rf\"%s(\\d+)\" %path.stem, d) for d in dirs]\n",
    "        i = [int(m.groups()[0]) for m in matches if m]\n",
    "        n = max(i) + 1 if i else 2\n",
    "        return f\"{path}{n}\"\n",
    "\n",
    "\n",
    "#Training model\n",
    "def train(model_dir, args):\n",
    "    seed_everything(args.seed)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"device(GPU) : {torch.cuda.is_available()}\")\n",
    "    num_classes = 2\n",
    "  \n",
    "    # Load model and use AutoTokenizer \n",
    "    #AutoTokenizer :A generic tokenizer class that will be instantiated as one of the tokenizer classes of the library\n",
    "    MODEL_NAME = args.pretrained_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Load Train dataset and validation dataset\n",
    "    train_dataset = load_data(\"./data/SKT_BoolQ_Train.tsv\")\n",
    "    val_dataset = load_data(\"./data/SKT_BoolQ_Dev.tsv\")\n",
    "\n",
    "    #Fix target(label) of train and validation dataset\n",
    "    train_label = train_dataset['label'].values\n",
    "    val_label = val_dataset['label'].values\n",
    "\n",
    "    #Tokenizing train and validation dataset\n",
    "    tokenized_train = tokenized_dataset(train_dataset, tokenizer, check_arch(args.model_type))\n",
    "    tokenized_val = tokenized_dataset(val_dataset, tokenizer, check_arch(args.model_type))\n",
    "\n",
    "    #Custon the dataset for experiment using pytorch\n",
    "    train_dataset = CustomDataset(tokenized_train, train_label)\n",
    "    val_dataset = CustomDataset(tokenized_val, val_label)\n",
    "    \n",
    "    #This is data loader to load the data \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.valid_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "  #Model hyperparameter setting \n",
    "    if args.model_type == 'Electra_BoolQ':\n",
    "        config_module = getattr(import_module(\"transformers\"), \"ElectraConfig\")\n",
    "    else:\n",
    "        config_module = getattr(import_module(\"transformers\"), args.model_type + \"Config\")\n",
    "  \n",
    "    model_config = config_module.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 2\n",
    "\n",
    "    model_module = eval(args.model_type)\n",
    "    \n",
    "    if args.custompretrain:\n",
    "        model = model_module.from_pretrained(args.custompretrain, args=args)\n",
    "        model = CustomPreTrainModel(config=model_config, model=model)\n",
    "    else:\n",
    "        if args.model_type in [\"BERT\", \"Electra\"]:\n",
    "            model = model_module.from_pretrained(MODEL_NAME, config=model_config, args=args)\n",
    "        else:\n",
    "            model = model_module(config=model_config, args=args)\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "    save_dir = increment_output_dir(os.path.join(model_dir, args.name, str(args.kfold)))\n",
    "\n",
    "  # Freeze Parameter\n",
    "    for name, param in model.named_parameters():\n",
    "        if ('cls_fc_layer' not in name) and ('label_classifier' not in name): # classifier layer\n",
    "            param.requires_grad = False\n",
    "\n",
    "  #Computing loss and useing optimizer module\n",
    "    criterion = create_criterion(args.criterion) \n",
    "    opt_module = getattr(import_module(\"transformers\"), args.optimizer)\n",
    "    optimizer = opt_module(\n",
    "          model.parameters(),\n",
    "          lr=args.lr,\n",
    "          weight_decay=args.weight_decay,\n",
    "          eps = 1e-8\n",
    "      )\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "      optimizer, \n",
    "      num_warmup_steps=args.warmup_steps, \n",
    "      num_training_steps=len(train_loader) * args.epochs, \n",
    "      last_epoch=- 1\n",
    "      )\n",
    "    start_time = time.time()\n",
    "    best_val_acc = -1\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(args.epochs):\n",
    "        pbar = tqdm(train_loader, dynamic_ncols=True)\n",
    "    # This is train loop \n",
    "    # We have to unfreeze the parameters\n",
    "        if epoch == args.freeze_epoch:\n",
    "            for name, param in model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, items in enumerate(pbar):\n",
    "            item = {key: val.to(device) for key, val in items.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outs = model(**item)\n",
    "            loss = criterion(outs[0], item['labels'])\n",
    "            preds = torch.argmax(outs[0], dim=-1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == item['labels']).sum().item()\n",
    "            if (idx + 1) % args.log_interval == 0:\n",
    "                train_loss = loss_value / args.log_interval\n",
    "                train_acc = matches / args.batch_size / args.log_interval\n",
    "                current_lr = get_lr(optimizer)\n",
    "                pbar.set_description(f\"Epoch: [{epoch}/{args.epochs}]({idx + 1}/{len(train_loader)}) || loss: {train_loss:4.4} || acc: {train_acc:4.2%} || lr {current_lr:4.4}\")\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "    # This is validation loop\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, dynamic_ncols=True)\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            acc_okay = 0\n",
    "            count_all = 0\n",
    "            TP = 0\n",
    "            FP = 0\n",
    "            TN = 0\n",
    "            FN = 0\n",
    "            eps = 1e-9\n",
    "            for idx, items in enumerate(pbar):\n",
    "                sleep(0.01)\n",
    "                item = {key: val.to(device) for key, val in items.items()}\n",
    "                outs = model(**item)\n",
    "                preds = torch.argmax(outs[0], dim=-1)\n",
    "                loss = criterion(outs[0], item['labels']).item()\n",
    "                acc_item = (item['labels'] == preds).sum().item()\n",
    "                TRUE = (item['labels'] == preds)\n",
    "                FALSE = (item['labels'] != preds)\n",
    "                TP += (TRUE * preds).sum().item()\n",
    "                TN += (TRUE * (preds==0)).sum().item()\n",
    "                FP += (FALSE * preds).sum().item()\n",
    "                FN += (FALSE * (preds==0)).sum().item()\n",
    "                val_loss_items.append(loss)\n",
    "                val_acc_items.append(acc_item)\n",
    "                acc_okay += acc_item\n",
    "                count_all += len(preds)\n",
    "                MCC = ((TP*TN) - (FP*FN)) / (((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**0.5)\n",
    "                pbar.set_description(f\"Epoch: [{epoch}/{args.epochs}]({idx + 1}/{len(val_loader)}) || val_loss: {loss:4.4} || acc: {acc_okay/count_all:4.2%} || MCC: {MCC:4.2%}\")\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loss_items)\n",
    "            val_acc = acc_okay / count_all\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                print(f\"New best model for val acc : {val_acc:4.2%}! saving the best model..\")\n",
    "                model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                model_to_save.save_pretrained(f\"{save_dir}/best\")\n",
    "                torch.save(args, os.path.join(f\"{save_dir}/best\", \"training_args.bin\"))\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            print(\n",
    "              f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.4}|| \"\n",
    "              f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.4}|| \"\n",
    "              f\"MCC : {MCC:4.2%}|| \"\n",
    "              f\"TP:{TP} / TN:{TN} / FP:{FP} / FN:{FN}\"\n",
    "            )\n",
    "\n",
    "            s = f'Time elapsed: {(time.time() - start_time)/60: .2f} min'\n",
    "            print(s)\n",
    "            print()\n",
    "            \n",
    "            if val_acc > 0:\n",
    "                path = './jihyunee'\n",
    "                name = '%s_판정의문문_%.6f.pt' % (path, val_acc)\n",
    "                torch.save(model.state_dict(), os.path.join('saved_model', name))\n",
    "\n",
    "\n",
    "\n",
    "#Set default parameters like epoch or batchsize\n",
    "if __name__ == '__main__':\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_type', type=str, default='BertBase')\n",
    "    parser.add_argument('--pretrained_model', type=str, default='bert-base-uncased')\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--freeze_epoch', type=int, default=0)\n",
    "    parser.add_argument('--batch_size', type=int, default=4)\n",
    "    parser.add_argument('--valid_batch_size', type=int, default=128)\n",
    "    parser.add_argument('--val_ratio', type=float, default=0.2, help='ratio for validaton (default: 0.2)')\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.1, help=\"Dropout for fully-connected layers\")\n",
    "    parser.add_argument('--criterion', type=str, default='cross_entropy', help='criterion type (default: cross_entropy)')\n",
    "    parser.add_argument('--optimizer', type=str, default='AdamW', help='optimizer type (default: AdamW)')  \n",
    "    parser.add_argument('--lr', type=float, default=1e-6)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.01)\n",
    "    parser.add_argument('--warmup_steps', type=int, default=500)               # number of warmup steps for learning rate scheduler\n",
    "    parser.add_argument('--seed' , type=int , default = 42, help='random seed (default: 42)')\n",
    "    parser.add_argument('--log_interval', type=int, default=20, help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--kfold', type=int, default=1, help='k-fold currunt step number')\n",
    "    parser.add_argument('--name', default='exp', help='model save at {SM_MODEL_DIR}/{name}')\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', './boolq_data_results/results'))\n",
    "    parser.add_argument('--custompretrain', type=str, default=\"\", help='Use custom pretrain : model dir')\n",
    "    args = parser.parse_args(\"\")\n",
    "\n",
    "    \n",
    "    #Set the number of epoch, type of model and batch_size\n",
    "    args.epochs = 10\n",
    "    args.model_type = \"Roberta\"\n",
    "    args.pretrained_model = \"klue/roberta-large\"\n",
    "    args.lr = 8e-6\n",
    "    args.batch_size = 4\n",
    "\n",
    "\n",
    "    i = 1\n",
    "    print('='*40)\n",
    "    print(f\"k-fold num : {i}\")\n",
    "    print('='*40)\n",
    "    args.kfold = i\n",
    "\n",
    "    args.name = f'{args.model_type}_{args.lr}_load_data'\n",
    "\n",
    "\n",
    "\n",
    "    #####################Training!!!!!#############################################\n",
    "    train(args.model_dir, args)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"./캡쳐.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = load_data(\"./data/SKT_BoolQ_Dev.tsv\")\n",
    "\n",
    "def inference(val_dataset,args, PATH):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        \n",
    "    config = getattr(import_module(\"transformers\"), args.model_type + \"Config\").from_pretrained(args.pretrained_model)\n",
    "    model_module = eval(args.model_type)\n",
    "    model = model_module(config=config, args=args)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)        \n",
    "    val_label = val_dataset['label'].values\n",
    "    \n",
    "    global tokenized_sentences_d\n",
    "    tokenized_val = tokenized_dataset(val_dataset, tokenizer, check_arch('Roberta'))\n",
    "    val = CustomDataset(tokenized_val, val_label)    \n",
    "    dataloader = DataLoader(val, batch_size=args.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    labels = []\n",
    "    preds = []\n",
    "   \n",
    "    for i, items in enumerate(tqdm(dataloader)):\n",
    "        item = {key: val.to(device) for key, val in items.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**item)\n",
    "    \n",
    "        logits = outputs[0]\n",
    "        m = nn.Softmax(dim=1)\n",
    "        logits = m(logits)\n",
    "        pred = torch.argmax(logits, 1)\n",
    "        \n",
    "        pred = pred.detach().cpu().numpy().tolist()\n",
    "        label = item['labels'].detach().cpu().numpy().tolist()\n",
    "        \n",
    "        preds += pred\n",
    "        labels += label\n",
    "        \n",
    "    val_acc=accuracy_score(labels, preds)\n",
    "    \n",
    "    print('val')\n",
    "    print('acc: %.4f'%(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 175/175 [00:14<00:00, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "acc: 0.8557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = 'saved_model/판정의문문_0.855714.pt'\n",
    "inference(val_dataset, args, PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
